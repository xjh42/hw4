{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4\n",
    "\n",
    "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset. \n",
    "\n",
    "As always, we will start by copying this notebook and getting the starting code.\n",
    "Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignmen\n",
    "\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /root/miniconda3/lib/python3.8/site-packages/pybind11/include (found version \"2.13.1\")\n",
      "-- Found cuda, building cuda backend\n",
      "Wed Aug 14 10:02:24 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:3E:00.0 Off |                  N/A |\n",
      "| 32%   30C    P8             24W /  250W |       1MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  7.5\n",
      "-- Configuring done (0.2s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /root/hw4/build\n",
      "make[1]: Entering directory '/root/hw4/build'\n",
      "make[2]: Entering directory '/root/hw4/build'\n",
      "make[3]: Entering directory '/root/hw4/build'\n",
      "make[3]: Leaving directory '/root/hw4/build'\n",
      "make[3]: Entering directory '/root/hw4/build'\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module /root/hw4/python/needle/backend_ndarray/ndarray_backend_cpu.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/root/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/root/hw4/build'\n",
      "[ 75%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
      "make[3]: Leaving directory '/root/hw4/build'\n",
      "make[3]: Entering directory '/root/hw4/build'\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module /root/hw4/python/needle/backend_ndarray/ndarray_backend_cuda.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/root/hw4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/root/hw4/build'\n",
      "make[1]: Leaving directory '/root/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./python\n",
      "env: NEEDLE_BACKEND=nd\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework. Also copy the solutions in `src/ndarray_backend_cpu.cc` and `src/ndarray_backend_cuda.cu` from homework 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ND Backend [10 pts]\n",
    "\n",
    "Recall that in homework 2, the `array_api` was imported as `numpy`. In this part, the goal is to write the necessary operations with `array_api` imported from the needle backend `NDArray` in `python/needle/backend_ndarray/ndarray.py`. Make sure to copy the solutions for `reshape`, `permute`, `broadcast_to` and `__getitem__` from homework 3.\n",
    "\n",
    "Fill in the following classes in `python/needle/ops_logarithmic.py` and `python/needle/ops_mathematic.py`:\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `Transpose`\n",
    "- `Reshape`\n",
    "- `BroadcastTo`\n",
    "- `Summation`\n",
    "- `MatMul`\n",
    "- `Negate`\n",
    "- `Log`\n",
    "- `Exp`\n",
    "- `ReLU`\n",
    "- `LogSumExp`\n",
    "- `Tanh` (new)\n",
    "- `Stack` (new)\n",
    "- `Split` (new)\n",
    "\n",
    "Note that for most of these, you already wrote the solutions in the previous homework and you should not change most part of your previous solution, if issues arise, please check if the `array_api` function used is supported in the needle backend. \n",
    "\n",
    "`TanhOp`, `Stack`, and `Split` are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
    "\n",
    "**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
    "\n",
    "**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-8.3.2, pluggy-1.5.0 -- /root/miniconda3/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /root/hw4\n",
      "plugins: anyio-3.6.2\n",
      "collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m    [  0%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m  [  1%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m    [  2%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m  [  3%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m   [  4%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m [  5%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m   [  5%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m [  6%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m   [  7%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m   [  9%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 11%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 12%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m [ 13%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 14%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 15%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 16%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 16%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 17%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 18%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 19%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 20%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 21%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 22%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 22%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[31m          [ 23%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[31m             [ 24%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[31m             [ 25%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[31m             [ 26%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[31m             [ 27%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[31m          [ 27%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[31m          [ 28%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[31m          [ 29%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[31m          [ 30%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[31m          [ 31%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[31m       [ 32%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 33%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 33%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 34%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_power[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 35%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                [ 36%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                [ 37%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 38%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_log[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 38%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                [ 39%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                [ 40%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 41%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_exp[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 42%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 43%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 44%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 44%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_relu[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 45%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 46%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 47%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 48%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 49%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 50%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 50%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 51%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 52%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 53%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 54%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 55%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 55%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 56%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 57%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 63%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 64%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 65%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 66%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m    [ 66%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[31m       [ 67%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[31m       [ 68%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[31m       [ 69%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m  [ 80%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m  [ 81%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 83%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 84%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 85%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 86%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 87%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 88%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 88%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 89%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 90%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 91%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 92%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 93%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 94%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 94%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 95%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 96%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 97%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 98%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 99%]\u001b[0m\n",
      "tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m       [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________ test_ewise_fn[cpu-shape0-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571b04550>, shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[1.7886285]]])\n",
      "B          = needle.Tensor([[[0.43650985]]])\n",
      "_A         = array([[[1.7886285]]], dtype=float32)\n",
      "_B         = array([[[0.43650985]]], dtype=float32)\n",
      "device     = cpu()\n",
      "fn         = <function <lambda> at 0x7f4571b04550>\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:42: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[1.7886285]]])\n",
      "        b          = needle.Tensor([[[0.43650985]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:337: in __truediv__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseDiv()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[[0.43650985]]])\n",
      "        self       = needle.Tensor([[[1.7886285]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[1.7886285]]]), needle.Tensor([[[0.43650985]]]))\n",
      "        self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f45719d4fa0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[1.7886285]]]), needle.Tensor([[[0.43650985]]]))\n",
      "        op         = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f45719d4fa0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f462ed706a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f462ed706a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f45719d4fa0>\n",
      "a = NDArray([[[1.7886285]]], device=cpu())\n",
      "b = NDArray([[[0.43650985]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.divide(a, b)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[1.7886285]]], device=cpu())\n",
      "b          = NDArray([[[0.43650985]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f45719d4fa0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:119: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_ewise_fn[cpu-shape1-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571b04550>, shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
      "   -0.47721803]\n",
      "  [-1.3138647   0.8846224... 0.48641446  0.8515189\n",
      "    0.48624933]\n",
      "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
      "   -1.7441102 ]]])\n",
      "B          = needle.Tensor([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
      "   -6.4841092e-01 -7.3746485e-01]\n",
      "  [-1.6...-01 -4.3999133e-01]\n",
      "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
      "    1.0672156e+00  6.4142066e-01]]])\n",
      "_A         = array([[[-0.27738822, -0.35475898, -0.08274148, -0.6270007 ,\n",
      "         -0.04381817, -0.47721803],\n",
      "        [-1.3138647 ,...933],\n",
      "        [-0.83423984,  1.3449924 , -0.6782127 ,  0.42643508,\n",
      "         -0.7533348 , -1.7441102 ]]], dtype=float32)\n",
      "_B         = array([[[ 2.2575027e-01,  2.8703517e-01, -7.7440962e-02,  2.7606851e-01,\n",
      "         -6.4841092e-01, -7.3746485e-01],\n",
      "   ...3349704e-01, -1.2892612e+00, -1.9829026e-01,  2.4575877e+00,\n",
      "          1.0672156e+00,  6.4142066e-01]]], dtype=float32)\n",
      "device     = cpu()\n",
      "fn         = <function <lambda> at 0x7f4571b04550>\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:42: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
      "   -0.47721803]\n",
      "  [-1.3138647   0.8846224... 0.48641446  0.8515189\n",
      "    0.48624933]\n",
      "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
      "   -1.7441102 ]]])\n",
      "        b          = needle.Tensor([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
      "   -6.4841092e-01 -7.3746485e-01]\n",
      "  [-1.6...-01 -4.3999133e-01]\n",
      "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
      "    1.0672156e+00  6.4142066e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:337: in __truediv__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseDiv()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
      "   -6.4841092e-01 -7.3746485e-01]\n",
      "  [-1.6...-01 -4.3999133e-01]\n",
      "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
      "    1.0672156e+00  6.4142066e-01]]])\n",
      "        self       = needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
      "   -0.47721803]\n",
      "  [-1.3138647   0.8846224... 0.48641446  0.8515189\n",
      "    0.48624933]\n",
      "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
      "   -1.7441102 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
      "   -0.47721803]\n",
      "  [-1.3138647   0.884622...01 -4.3999133e-01]\n",
      "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
      "    1.0672156e+00  6.4142066e-01]]]))\n",
      "        self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f4571340550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
      "   -0.47721803]\n",
      "  [-1.3138647   0.884622...01 -4.3999133e-01]\n",
      "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
      "    1.0672156e+00  6.4142066e-01]]]))\n",
      "        op         = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f4571340550>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f4571340610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f4571340610>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f4571340550>\n",
      "a = NDArray([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
      "   -0.47721803]\n",
      "  [-1.3138647   0.8846224   0.8....8515189\n",
      "    0.48624933]\n",
      "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
      "   -1.7441102 ]]], device=cpu())\n",
      "b = NDArray([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
      "   -6.4841092e-01 -7.3746485e-01]\n",
      "  [-1.6809011...e-01]\n",
      "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
      "    1.0672156e+00  6.4142066e-01]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.divide(a, b)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.27738822 -0.35475898 -0.08274148 -0.6270007  -0.04381817\n",
      "   -0.47721803]\n",
      "  [-1.3138647   0.8846224   0.8....8515189\n",
      "    0.48624933]\n",
      "  [-0.83423984  1.3449924  -0.6782127   0.42643508 -0.7533348\n",
      "   -1.7441102 ]]], device=cpu())\n",
      "b          = NDArray([[[ 2.2575027e-01  2.8703517e-01 -7.7440962e-02  2.7606851e-01\n",
      "   -6.4841092e-01 -7.3746485e-01]\n",
      "  [-1.6809011...e-01]\n",
      "  [ 1.3349704e-01 -1.2892612e+00 -1.9829026e-01  2.4575877e+00\n",
      "    1.0672156e+00  6.4142066e-01]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f4571340550>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:119: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_ewise_fn[cuda-shape0-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571b04550>, shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.34836832]]])\n",
      "B          = needle.Tensor([[[-1.8110192]]])\n",
      "_A         = array([[[-0.34836832]]], dtype=float32)\n",
      "_B         = array([[[-1.8110192]]], dtype=float32)\n",
      "device     = cuda()\n",
      "fn         = <function <lambda> at 0x7f4571b04550>\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:42: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.34836832]]])\n",
      "        b          = needle.Tensor([[[-1.8110192]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:337: in __truediv__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseDiv()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[[-1.8110192]]])\n",
      "        self       = needle.Tensor([[[-0.34836832]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.34836832]]]), needle.Tensor([[[-1.8110192]]]))\n",
      "        self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f45713707f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.34836832]]]), needle.Tensor([[[-1.8110192]]]))\n",
      "        op         = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f45713707f0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f4571370850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f4571370850>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f45713707f0>\n",
      "a = NDArray([[[-0.34836832]]], device=cuda())\n",
      "b = NDArray([[[-1.8110192]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.divide(a, b)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.34836832]]], device=cuda())\n",
      "b          = NDArray([[[-1.8110192]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f45713707f0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:119: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_ewise_fn[cuda-shape1-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571b04550>, shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
      "   -0.34903246]\n",
      "  [-1.0753901   1.3716581 ...-0.8066985   1.6680253\n",
      "    0.25968838]\n",
      "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
      "    0.33900097]]])\n",
      "B          = needle.Tensor([[[ 0.14525874 -0.9536581   0.6435548   0.20046894  0.00906379\n",
      "   -0.56320494]\n",
      "  [ 0.85617113  1.2471844...1.4773219   0.37959117\n",
      "    1.3136121 ]\n",
      "  [-1.350194    1.6009326   0.2970727  -0.2077485  -1.8044882\n",
      "    1.087597  ]]])\n",
      "_A         = array([[[-1.4833167 ,  0.13791604,  1.1932526 , -1.0757235 ,\n",
      "          1.7676828 , -0.34903246],\n",
      "        [-1.0753901 ,...838],\n",
      "        [ 0.11024354,  0.57954335, -1.7123466 ,  0.42849943,\n",
      "          0.9961082 ,  0.33900097]]], dtype=float32)\n",
      "_B         = array([[[ 0.14525874, -0.9536581 ,  0.6435548 ,  0.20046894,\n",
      "          0.00906379, -0.56320494],\n",
      "        [ 0.85617113,...21 ],\n",
      "        [-1.350194  ,  1.6009326 ,  0.2970727 , -0.2077485 ,\n",
      "         -1.8044882 ,  1.087597  ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "fn         = <function <lambda> at 0x7f4571b04550>\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:42: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
      "   -0.34903246]\n",
      "  [-1.0753901   1.3716581 ...-0.8066985   1.6680253\n",
      "    0.25968838]\n",
      "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
      "    0.33900097]]])\n",
      "        b          = needle.Tensor([[[ 0.14525874 -0.9536581   0.6435548   0.20046894  0.00906379\n",
      "   -0.56320494]\n",
      "  [ 0.85617113  1.2471844...1.4773219   0.37959117\n",
      "    1.3136121 ]\n",
      "  [-1.350194    1.6009326   0.2970727  -0.2077485  -1.8044882\n",
      "    1.087597  ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:337: in __truediv__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.EWiseDiv()(\u001b[96mself\u001b[39;49;00m, other)\u001b[90m\u001b[39;49;00m\n",
      "        other      = needle.Tensor([[[ 0.14525874 -0.9536581   0.6435548   0.20046894  0.00906379\n",
      "   -0.56320494]\n",
      "  [ 0.85617113  1.2471844...1.4773219   0.37959117\n",
      "    1.3136121 ]\n",
      "  [-1.350194    1.6009326   0.2970727  -0.2077485  -1.8044882\n",
      "    1.087597  ]]])\n",
      "        self       = needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
      "   -0.34903246]\n",
      "  [-1.0753901   1.3716581 ...-0.8066985   1.6680253\n",
      "    0.25968838]\n",
      "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
      "    0.33900097]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
      "   -0.34903246]\n",
      "  [-1.0753901   1.3716581....4773219   0.37959117\n",
      "    1.3136121 ]\n",
      "  [-1.350194    1.6009326   0.2970727  -0.2077485  -1.8044882\n",
      "    1.087597  ]]]))\n",
      "        self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f457133c790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
      "   -0.34903246]\n",
      "  [-1.0753901   1.3716581....4773219   0.37959117\n",
      "    1.3136121 ]\n",
      "  [-1.350194    1.6009326   0.2970727  -0.2077485  -1.8044882\n",
      "    1.087597  ]]]))\n",
      "        op         = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f457133c790>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f457133c550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f457133c550>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f457133c790>\n",
      "a = NDArray([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
      "   -0.34903246]\n",
      "  [-1.0753901   1.3716581   0.29...6680253\n",
      "    0.25968838]\n",
      "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
      "    0.33900097]]], device=cuda())\n",
      "b = NDArray([[[ 0.14525874 -0.9536581   0.6435548   0.20046894  0.00906379\n",
      "   -0.56320494]\n",
      "  [ 0.85617113  1.2471844   0.3...7959117\n",
      "    1.3136121 ]\n",
      "  [-1.350194    1.6009326   0.2970727  -0.2077485  -1.8044882\n",
      "    1.087597  ]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a, b):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.divide(a, b)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.4833167   0.13791604  1.1932526  -1.0757235   1.7676828\n",
      "   -0.34903246]\n",
      "  [-1.0753901   1.3716581   0.29...6680253\n",
      "    0.25968838]\n",
      "  [ 0.11024354  0.57954335 -1.7123466   0.42849943  0.9961082\n",
      "    0.33900097]]], device=cuda())\n",
      "b          = NDArray([[[ 0.14525874 -0.9536581   0.6435548   0.20046894  0.00906379\n",
      "   -0.56320494]\n",
      "  [ 0.85617113  1.2471844   0.3...7959117\n",
      "    1.3136121 ]\n",
      "  [-1.350194    1.6009326   0.2970727  -0.2077485  -1.8044882\n",
      "    1.087597  ]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.EWiseDiv object at 0x7f457133c790>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:119: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_scalar_fn[cpu-shape0-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571a00310>, shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.4221624]]])\n",
      "_A         = array([[[-1.4221624]]], dtype=float32)\n",
      "_B         = -0.019735336303710938\n",
      "device     = cpu()\n",
      "fn         = <function <lambda> at 0x7f4571a00310>\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.4221624]]])\n",
      "        b          = -0.019735336303710938\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:339: in __truediv__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = -0.019735336303710938\n",
      "        self       = needle.Tensor([[[-1.4221624]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.4221624]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f45713405e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.4221624]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.DivScalar object at 0x7f45713405e0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f457139cdc0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f457139cdc0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.DivScalar object at 0x7f45713405e0>\n",
      "a = NDArray([[[-1.4221624]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        scaler = \u001b[96mself\u001b[39;49;00m.scalar\u001b[90m\u001b[39;49;00m\n",
      ">       out = array_api.divide(a, scaler, dtype=a.dtype)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.4221624]]], device=cpu())\n",
      "scaler     = -0.019735336303710938\n",
      "self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f45713405e0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:141: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_scalar_fn[cpu-shape0-subtract] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571a003a0>, shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.6289013]]])\n",
      "_A         = array([[[0.6289013]]], dtype=float32)\n",
      "_B         = -0.023876570165157318\n",
      "device     = cpu()\n",
      "fn         = <function <lambda> at 0x7f4571a003a0>\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:61: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33msubtract\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a - b\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.6289013]]])\n",
      "        b          = -0.023876570165157318\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:333: in __sub__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.AddScalar(-other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = -0.023876570165157318\n",
      "        self       = needle.Tensor([[[0.6289013]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.6289013]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.AddScalar object at 0x7f457133c2e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.6289013]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.AddScalar object at 0x7f457133c2e0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'add'\") raised in repr()] Tensor object at 0x7f457133caf0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'add'\") raised in repr()] Tensor object at 0x7f457133caf0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.AddScalar object at 0x7f457133c2e0>\n",
      "a = NDArray([[[0.6289013]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.add(a, \u001b[96mself\u001b[39;49;00m.scalar, dtype=a.dtype)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'add'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.6289013]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.AddScalar object at 0x7f457133c2e0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:34: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_scalar_fn[cpu-shape1-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571a00310>, shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022....44244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]])\n",
      "_A         = array([[[-0.15259542, -0.03079642,  1.289313  ,  0.5304533 ,\n",
      "         -0.74591106, -0.28878465],\n",
      "        [ 1.204689  ,...64 ],\n",
      "        [-1.2044502 ,  0.9504057 , -0.3747067 , -0.3207342 ,\n",
      "         -0.98206496, -1.7369602 ]]], dtype=float32)\n",
      "_B         = -1.2508771419525146\n",
      "device     = cpu()\n",
      "fn         = <function <lambda> at 0x7f4571a00310>\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022....44244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]])\n",
      "        b          = -1.2508771419525146\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:339: in __truediv__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = -1.2508771419525146\n",
      "        self       = needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022....44244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.580302...4244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f4571263580>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.580302...4244337  0.15791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.DivScalar object at 0x7f4571263580>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f4571263640>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f4571263640>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.DivScalar object at 0x7f4571263580>\n",
      "a = NDArray([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022  -0.3...5791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        scaler = \u001b[96mself\u001b[39;49;00m.scalar\u001b[90m\u001b[39;49;00m\n",
      ">       out = array_api.divide(a, scaler, dtype=a.dtype)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.15259542 -0.03079642  1.289313    0.5304533  -0.74591106\n",
      "   -0.28878465]\n",
      "  [ 1.204689   -0.5803022  -0.3...5791912\n",
      "    1.3491064 ]\n",
      "  [-1.2044502   0.9504057  -0.3747067  -0.3207342  -0.98206496\n",
      "   -1.7369602 ]]], device=cpu())\n",
      "scaler     = -1.2508771419525146\n",
      "self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f4571263580>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:141: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_scalar_fn[cpu-shape1-subtract] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571a003a0>, shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 1.5867459e+00 -1.2319332e+00 -4.7803262e-01  1.0614400e+00\n",
      "   -2.1596379e+00  3.9598548e-01]\n",
      "  [-5.6...-01 -5.3766370e-01]\n",
      "  [ 6.3884163e-01  6.0558505e-02  7.4565703e-01  2.1174929e-01\n",
      "    7.5366856e-03  3.8789740e-01]]])\n",
      "_A         = array([[[ 1.5867459e+00, -1.2319332e+00, -4.7803262e-01,  1.0614400e+00,\n",
      "         -2.1596379e+00,  3.9598548e-01],\n",
      "   ...3884163e-01,  6.0558505e-02,  7.4565703e-01,  2.1174929e-01,\n",
      "          7.5366856e-03,  3.8789740e-01]]], dtype=float32)\n",
      "_B         = -1.5932109355926514\n",
      "device     = cpu()\n",
      "fn         = <function <lambda> at 0x7f4571a003a0>\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:61: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33msubtract\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a - b\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 1.5867459e+00 -1.2319332e+00 -4.7803262e-01  1.0614400e+00\n",
      "   -2.1596379e+00  3.9598548e-01]\n",
      "  [-5.6...-01 -5.3766370e-01]\n",
      "  [ 6.3884163e-01  6.0558505e-02  7.4565703e-01  2.1174929e-01\n",
      "    7.5366856e-03  3.8789740e-01]]])\n",
      "        b          = -1.5932109355926514\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:333: in __sub__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.AddScalar(-other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = -1.5932109355926514\n",
      "        self       = needle.Tensor([[[ 1.5867459e+00 -1.2319332e+00 -4.7803262e-01  1.0614400e+00\n",
      "   -2.1596379e+00  3.9598548e-01]\n",
      "  [-5.6...-01 -5.3766370e-01]\n",
      "  [ 6.3884163e-01  6.0558505e-02  7.4565703e-01  2.1174929e-01\n",
      "    7.5366856e-03  3.8789740e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 1.5867459e+00 -1.2319332e+00 -4.7803262e-01  1.0614400e+00\n",
      "   -2.1596379e+00  3.9598548e-01]\n",
      "  [-5....1 -5.3766370e-01]\n",
      "  [ 6.3884163e-01  6.0558505e-02  7.4565703e-01  2.1174929e-01\n",
      "    7.5366856e-03  3.8789740e-01]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.AddScalar object at 0x7f45713205b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 1.5867459e+00 -1.2319332e+00 -4.7803262e-01  1.0614400e+00\n",
      "   -2.1596379e+00  3.9598548e-01]\n",
      "  [-5....1 -5.3766370e-01]\n",
      "  [ 6.3884163e-01  6.0558505e-02  7.4565703e-01  2.1174929e-01\n",
      "    7.5366856e-03  3.8789740e-01]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.AddScalar object at 0x7f45713205b0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'add'\") raised in repr()] Tensor object at 0x7f45713206a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'add'\") raised in repr()] Tensor object at 0x7f45713206a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.AddScalar object at 0x7f45713205b0>\n",
      "a = NDArray([[[ 1.5867459e+00 -1.2319332e+00 -4.7803262e-01  1.0614400e+00\n",
      "   -2.1596379e+00  3.9598548e-01]\n",
      "  [-5.6200451...e-01]\n",
      "  [ 6.3884163e-01  6.0558505e-02  7.4565703e-01  2.1174929e-01\n",
      "    7.5366856e-03  3.8789740e-01]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.add(a, \u001b[96mself\u001b[39;49;00m.scalar, dtype=a.dtype)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'add'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 1.5867459e+00 -1.2319332e+00 -4.7803262e-01  1.0614400e+00\n",
      "   -2.1596379e+00  3.9598548e-01]\n",
      "  [-5.6200451...e-01]\n",
      "  [ 6.3884163e-01  6.0558505e-02  7.4565703e-01  2.1174929e-01\n",
      "    7.5366856e-03  3.8789740e-01]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.AddScalar object at 0x7f45713205b0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:34: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_scalar_fn[cuda-shape0-divide] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571a00310>, shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.21264285]]])\n",
      "_A         = array([[[-0.21264285]]], dtype=float32)\n",
      "_B         = -0.6414637565612793\n",
      "device     = cuda()\n",
      "fn         = <function <lambda> at 0x7f4571a00310>\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.21264285]]])\n",
      "        b          = -0.6414637565612793\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:339: in __truediv__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = -0.6414637565612793\n",
      "        self       = needle.Tensor([[[-0.21264285]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.21264285]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f462ed4bfa0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.21264285]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.DivScalar object at 0x7f462ed4bfa0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f462ed70670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f462ed70670>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.DivScalar object at 0x7f462ed4bfa0>\n",
      "a = NDArray([[[-0.21264285]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        scaler = \u001b[96mself\u001b[39;49;00m.scalar\u001b[90m\u001b[39;49;00m\n",
      ">       out = array_api.divide(a, scaler, dtype=a.dtype)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.21264285]]], device=cuda())\n",
      "scaler     = -0.6414637565612793\n",
      "self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f462ed4bfa0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:141: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_scalar_fn[cuda-shape0-subtract] _____________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571a003a0>, shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.7919234]]])\n",
      "_A         = array([[[0.7919234]]], dtype=float32)\n",
      "_B         = -0.7252244353294373\n",
      "device     = cuda()\n",
      "fn         = <function <lambda> at 0x7f4571a003a0>\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:61: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33msubtract\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a - b\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.7919234]]])\n",
      "        b          = -0.7252244353294373\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:333: in __sub__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.AddScalar(-other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = -0.7252244353294373\n",
      "        self       = needle.Tensor([[[0.7919234]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7919234]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.AddScalar object at 0x7f45712465e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.7919234]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.AddScalar object at 0x7f45712465e0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'add'\") raised in repr()] Tensor object at 0x7f4571246100>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'add'\") raised in repr()] Tensor object at 0x7f4571246100>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.AddScalar object at 0x7f45712465e0>\n",
      "a = NDArray([[[0.7919234]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.add(a, \u001b[96mself\u001b[39;49;00m.scalar, dtype=a.dtype)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'add'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.7919234]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.AddScalar object at 0x7f45712465e0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:34: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_scalar_fn[cuda-shape1-divide] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571a00310>, shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.7119254...0.0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]])\n",
      "_A         = array([[[ 0.17852722, -0.48561108,  0.4178631 ,  0.6601184 ,\n",
      "         -0.26862556,  0.04397374],\n",
      "        [ 0.11434969,...873],\n",
      "        [ 0.95702404,  0.3420432 , -1.1888571 , -1.3259709 ,\n",
      "         -1.4491868 ,  0.3708898 ]]], dtype=float32)\n",
      "_B         = -0.7234068512916565\n",
      "device     = cuda()\n",
      "fn         = <function <lambda> at 0x7f4571a00310>\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:60: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.7119254...0.0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]])\n",
      "        b          = -0.7234068512916565\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:339: in __truediv__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.DivScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = -0.7234068512916565\n",
      "        self       = needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.7119254...0.0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.711925...0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f45713d9430>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.711925...0763792   0.36092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.DivScalar object at 0x7f45713d9430>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f45713d9610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'divide'\") raised in repr()] Tensor object at 0x7f45713d9610>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.DivScalar object at 0x7f45713d9430>\n",
      "a = NDArray([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.71192545 -0.3...6092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        scaler = \u001b[96mself\u001b[39;49;00m.scalar\u001b[90m\u001b[39;49;00m\n",
      ">       out = array_api.divide(a, scaler, dtype=a.dtype)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 0.17852722 -0.48561108  0.4178631   0.6601184  -0.26862556\n",
      "    0.04397374]\n",
      "  [ 0.11434969  0.71192545 -0.3...6092362\n",
      "   -0.31638873]\n",
      "  [ 0.95702404  0.3420432  -1.1888571  -1.3259709  -1.4491868\n",
      "    0.3708898 ]]], device=cuda())\n",
      "scaler     = -0.7234068512916565\n",
      "self       = <needle.ops.ops_mathematic.DivScalar object at 0x7f45713d9430>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:141: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_scalar_fn[cuda-shape1-subtract] _____________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x7f4571a003a0>, shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.37743163 -0.0939057   1.2136433   2.1777341  -0.12863235\n",
      "    1.989526  ]\n",
      "  [ 0.5402597   1.2015828....00704432  0.38655233\n",
      "    0.24523386]\n",
      "  [-1.2373476   0.6722726  -1.3963248   2.26083    -0.72201365\n",
      "   -0.01691202]]])\n",
      "_A         = array([[[-0.37743163, -0.0939057 ,  1.2136433 ,  2.1777341 ,\n",
      "         -0.12863235,  1.989526  ],\n",
      "        [ 0.5402597 ,...386],\n",
      "        [-1.2373476 ,  0.6722726 , -1.3963248 ,  2.26083   ,\n",
      "         -0.72201365, -0.01691202]]], dtype=float32)\n",
      "_B         = 1.3843916654586792\n",
      "device     = cuda()\n",
      "fn         = <function <lambda> at 0x7f4571a003a0>\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:61: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33msubtract\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a - b\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.37743163 -0.0939057   1.2136433   2.1777341  -0.12863235\n",
      "    1.989526  ]\n",
      "  [ 0.5402597   1.2015828....00704432  0.38655233\n",
      "    0.24523386]\n",
      "  [-1.2373476   0.6722726  -1.3963248   2.26083    -0.72201365\n",
      "   -0.01691202]]])\n",
      "        b          = 1.3843916654586792\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:333: in __sub__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.AddScalar(-other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = 1.3843916654586792\n",
      "        self       = needle.Tensor([[[-0.37743163 -0.0939057   1.2136433   2.1777341  -0.12863235\n",
      "    1.989526  ]\n",
      "  [ 0.5402597   1.2015828....00704432  0.38655233\n",
      "    0.24523386]\n",
      "  [-1.2373476   0.6722726  -1.3963248   2.26083    -0.72201365\n",
      "   -0.01691202]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.37743163 -0.0939057   1.2136433   2.1777341  -0.12863235\n",
      "    1.989526  ]\n",
      "  [ 0.5402597   1.201582...0704432  0.38655233\n",
      "    0.24523386]\n",
      "  [-1.2373476   0.6722726  -1.3963248   2.26083    -0.72201365\n",
      "   -0.01691202]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.AddScalar object at 0x7f462ec81b20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.37743163 -0.0939057   1.2136433   2.1777341  -0.12863235\n",
      "    1.989526  ]\n",
      "  [ 0.5402597   1.201582...0704432  0.38655233\n",
      "    0.24523386]\n",
      "  [-1.2373476   0.6722726  -1.3963248   2.26083    -0.72201365\n",
      "   -0.01691202]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.AddScalar object at 0x7f462ec81b20>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'add'\") raised in repr()] Tensor object at 0x7f462ec81e20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'add'\") raised in repr()] Tensor object at 0x7f462ec81e20>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.AddScalar object at 0x7f462ec81b20>\n",
      "a = NDArray([[[-0.37743163 -0.0939057   1.2136433   2.1777341  -0.12863235\n",
      "    1.989526  ]\n",
      "  [ 0.5402597   1.2015828  -0.2...655233\n",
      "    0.24523386]\n",
      "  [-1.2373476   0.6722726  -1.3963248   2.26083    -0.72201365\n",
      "   -0.01691202]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.add(a, \u001b[96mself\u001b[39;49;00m.scalar, dtype=a.dtype)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'add'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.37743163 -0.0939057   1.2136433   2.1777341  -0.12863235\n",
      "    1.989526  ]\n",
      "  [ 0.5402597   1.2015828  -0.2...655233\n",
      "    0.24523386]\n",
      "  [-1.2373476   0.6722726  -1.3963248   2.26083    -0.72201365\n",
      "   -0.01691202]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.AddScalar object at 0x7f462ec81b20>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:34: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_power[cpu-shape0] ____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.9513445]]])\n",
      "_A         = array([[[-0.9513445]]], dtype=float32)\n",
      "_B         = 0\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:102: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:327: in __pow__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = 0\n",
      "        self       = needle.Tensor([[[-0.9513445]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.9513445]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45713d9d00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.9513445]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45713d9d00>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f45713d9670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f45713d9670>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45713d9d00>\n",
      "a = NDArray([[[-0.9513445]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.power(a, \u001b[96mself\u001b[39;49;00m.scalar)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.9513445]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45713d9d00>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:80: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_power[cpu-shape1] ____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9.0...+00  5.7307726e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]])\n",
      "_A         = array([[[-5.0679171e-01,  3.2509923e-01, -9.7235195e-02,  8.6115736e-01,\n",
      "          1.5928187e+00, -1.6549641e-01],\n",
      "   ...1119346e-01,  3.5744989e-01, -5.4865015e-01, -1.9209417e+00,\n",
      "          3.4405407e-01, -3.6635080e-01]]], dtype=float32)\n",
      "_B         = 0\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:102: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:327: in __pow__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = 0\n",
      "        self       = needle.Tensor([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9.0...+00  5.7307726e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9....0  5.7307726e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f457136b0d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9....0  5.7307726e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.PowerScalar object at 0x7f457136b0d0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f457136b1c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f457136b1c0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.PowerScalar object at 0x7f457136b0d0>\n",
      "a = NDArray([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9.0205765...e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.power(a, \u001b[96mself\u001b[39;49;00m.scalar)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-5.0679171e-01  3.2509923e-01 -9.7235195e-02  8.6115736e-01\n",
      "    1.5928187e+00 -1.6549641e-01]\n",
      "  [ 9.0205765...e-01]\n",
      "  [-4.1119346e-01  3.5744989e-01 -5.4865015e-01 -1.9209417e+00\n",
      "    3.4405407e-01 -3.6635080e-01]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f457136b0d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:80: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_power[cuda-shape0] ____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[1.171145]]])\n",
      "_A         = array([[[1.171145]]], dtype=float32)\n",
      "_B         = 0\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:102: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:327: in __pow__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = 0\n",
      "        self       = needle.Tensor([[[1.171145]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[1.171145]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45704645e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[1.171145]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45704645e0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f45704645b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f45704645b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45704645e0>\n",
      "a = NDArray([[[1.171145]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.power(a, \u001b[96mself\u001b[39;49;00m.scalar)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[1.171145]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45704645e0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:80: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_power[cuda-shape1] ____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_power\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.9756789...0.01338127  0.37821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]])\n",
      "_A         = array([[[-0.6280028 , -0.43987197,  2.9139524 ,  0.05009432,\n",
      "         -0.73577106, -0.47775924],\n",
      "        [-1.7431105 ,...16 ],\n",
      "        [-0.8517918 , -1.9387237 ,  0.42015556, -0.24831858,\n",
      "         -1.0194412 ,  0.38210192]]], dtype=float32)\n",
      "_B         = 0\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:102: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:327: in __pow__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.PowerScalar(other)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        other      = 0\n",
      "        self       = needle.Tensor([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.9756789...0.01338127  0.37821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.975678...01338127  0.37821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45712a1730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.975678...01338127  0.37821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45712a1730>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f45712a15b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'power'\") raised in repr()] Tensor object at 0x7f45712a15b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45712a1730>\n",
      "a = NDArray([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.9756789  -0.3...7821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a: NDArray) -> NDArray:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.power(a, \u001b[96mself\u001b[39;49;00m.scalar)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.6280028  -0.43987197  2.9139524   0.05009432 -0.73577106\n",
      "   -0.47775924]\n",
      "  [-1.7431105  -1.9756789  -0.3...7821808\n",
      "    1.2529916 ]\n",
      "  [-0.8517918  -1.9387237   0.42015556 -0.24831858 -1.0194412\n",
      "    0.38210192]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.PowerScalar object at 0x7f45712a1730>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:80: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cpu-shape0] _____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.333942]]])\n",
      "_A         = array([[[-1.333942]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:353: in tanh\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.333942]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.333942]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f45713769a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.333942]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f45713769a0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713768b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713768b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Tanh object at 0x7f45713769a0>\n",
      "a = NDArray([[[-1.333942]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.333942]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f45713769a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cpu-shape1] _____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      " ...50593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]])\n",
      "_A         = array([[[ 4.68197703e-01,  2.24348330e+00,  4.72604215e-01,\n",
      "          1.18212029e-01, -6.31235898e-01,  1.17569625e+00...,  1.98646748e+00, -1.20881248e+00,\n",
      "          8.64627600e-01,  1.01337186e-03, -9.01962698e-01]]],\n",
      "      dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:353: in tanh\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      " ...50593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "...593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f4571393f40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "...593936e-01]\n",
      "  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f4571393f40>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713935e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713935e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Tanh object at 0x7f4571393f40>\n",
      "a = NDArray([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "  [-5.4...  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 4.68197703e-01  2.24348330e+00  4.72604215e-01  1.18212029e-01\n",
      "   -6.31235898e-01  1.17569625e+00]\n",
      "  [-5.4...  [ 1.07073344e-01  1.98646748e+00 -1.20881248e+00  8.64627600e-01\n",
      "    1.01337186e-03 -9.01962698e-01]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f4571393f40>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape0] ____________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-2.3530958]]])\n",
      "_A         = array([[[-2.3530958]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:353: in tanh\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-2.3530958]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-2.3530958]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f45704763a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-2.3530958]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f45704763a0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45704766a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45704766a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Tanh object at 0x7f45704763a0>\n",
      "a = NDArray([[[-2.3530958]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-2.3530958]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f45704763a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape1] ____________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628...0.8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]])\n",
      "_A         = array([[[-0.5291037 , -1.1316243 ,  1.2829843 ,  0.6779355 ,\n",
      "         -0.32166547, -0.51734763],\n",
      "        [ 0.48438653,...48 ],\n",
      "        [-0.8919766 , -0.5311779 ,  1.3061328 ,  0.70647854,\n",
      "          0.99664706, -1.344323  ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:353: in tanh\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628...0.8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.173262...8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f4571354610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.173262...8592022   0.2965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f4571354610>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f4571354760>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f4571354760>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Tanh object at 0x7f4571354610>\n",
      "a = NDArray([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628   1.5...965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.5291037  -1.1316243   1.2829843   0.6779355  -0.32166547\n",
      "   -0.51734763]\n",
      "  [ 0.48438653 -1.1732628   1.5...965716\n",
      "   -1.0003148 ]\n",
      "  [-0.8919766  -0.5311779   1.3061328   0.70647854  0.99664706\n",
      "   -1.344323  ]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f4571354610>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: NotImplementedError\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.5997132]]])\n",
      "_A         = array([[[-1.5997132]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7f460fbabdc0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:353: in tanh\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.5997132]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f45712af280>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f45712af280>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712af1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712af1f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Tanh object at 0x7f45712af280>\n",
      "a = NDArray([[[-1.5997132]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.5997132]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f45712af280>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: NotImplementedError\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119   ... -0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]])\n",
      "_A         = array([[[ 1.6233783 , -0.58456963,  0.86539763,  0.8150671 ,\n",
      "          0.52470696, -2.4929152 ],\n",
      "        [ 0.28299394,...27 ],\n",
      "        [ 0.07837614, -0.39979827, -0.04655599, -1.3993185 ,\n",
      "          2.621195  , -0.13064201]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119  ...0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7f460fbabdc0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:353: in tanh\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119   ... -0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119  ...0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f45712f1790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119  ...0.04531233 -0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f45712f1790>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712f1730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712f1730>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Tanh object at 0x7f45712f1790>\n",
      "a = NDArray([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119     -1.4...0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 1.6233783  -0.58456963  0.86539763  0.8150671   0.52470696\n",
      "   -2.4929152 ]\n",
      "  [ 0.28299394  0.8119     -1.4...0.8650645\n",
      "   -1.3442827 ]\n",
      "  [ 0.07837614 -0.39979827 -0.04655599 -1.3993185   2.621195\n",
      "   -0.13064201]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f45712f1790>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.2775043]]])\n",
      "_A         = array([[[-1.2775043]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.2775043]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7f460fbabdc0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:353: in tanh\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.2775043]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.2775043]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f4571393ac0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.2775043]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f4571393ac0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713937f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713937f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Tanh object at 0x7f4571393ac0>\n",
      "a = NDArray([[[-1.2775043]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.2775043]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f4571393ac0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "_A         = array([[[-0.71145284,  0.96028835, -0.01700081,  0.18537076,\n",
      "         -0.92995864,  1.2871389 ],\n",
      "        [-0.69594264,...502],\n",
      "        [-0.03949696, -0.9310424 , -0.55855966, -1.2472047 ,\n",
      "         -1.1149031 , -0.5235176 ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n",
      "    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.697433....02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]]),)\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7f460fbabdc0>\n",
      "        kwargs     = {}\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:353: in tanh\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tanh()(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.697433....02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Tanh object at 0x7f4571287640>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.697433....02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Tanh object at 0x7f4571287640>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f4571287df0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f4571287df0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Tanh object at 0x7f4571287640>\n",
      "a = NDArray([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334   0.7...3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334   0.7...3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Tanh object at 0x7f4571287640>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:343: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1...7868 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]])]\n",
      "A_t        = [tensor([[ 0.9080,  1.1915,  0.6277,  1.7709,  2.5803],\n",
      "        [ 1.0994, -0.2561, -0.3035, -1.2755,  1.4340],\n",
      "       ....9323],\n",
      "        [-0.1727, -0.3860,  0.6065,  0.3832, -1.2306],\n",
      "        [-0.7605,  1.3836, -1.3960, -0.2341,  0.9361]])]\n",
      "_A         = [array([[ 0.90795034,  1.1914672 ,  0.6276671 ,  1.7709465 ,  2.5802875 ],\n",
      "       [ 1.0994289 , -0.25607443, -0.303511...8323304, -1.2305677 ],\n",
      "       [-0.76054263,  1.3835862 , -1.396009  , -0.23408563,  0.9360824 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1...7868 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.2560...8 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]]),),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f45703fe550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.2560...8 -0.38599226  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]]),),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f45703fe550>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45703fe580>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45703fe580>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f45703fe550>\n",
      "args = (NDArray([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1.27553...6  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]], device=cpu()),)\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[ 0.90795034  1.1914672   0.6276671   1.7709465   2.5802875 ]\n",
      " [ 1.0994289  -0.25607443 -0.30351138 -1.27553...6  0.6064613   0.38323304 -1.2305677 ]\n",
      " [-0.76054263  1.3835862  -1.396009   -0.23408563  0.9360824 ]], device=cpu()),)\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f45703fe550>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0...071  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])]\n",
      "A_t        = [tensor([[ 0.2389,  1.7993,  0.1729, -0.1885,  1.6104],\n",
      "        [-0.5798, -1.7727,  0.7770, -0.6439,  0.3592],\n",
      "       ....3381],\n",
      "        [-1.3337, -0.5883, -1.3194, -2.0161,  0.7420],\n",
      "        [-1.2923,  0.1274, -1.2252,  1.4211,  1.0188]])]\n",
      "_A         = [array([[ 0.23885646,  1.7992946 ,  0.17290778, -0.18845753,  1.6103987 ],\n",
      "       [-0.57978904, -1.7727456 ,  0.776984...160983 ,  0.74200845],\n",
      "       [-1.2923242 ,  0.1273649 , -1.225224  ,  1.421074  ,  1.0188165 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0...071  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727...1  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f45703faa60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727...1  -0.5883452  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]])),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f45703faa60>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45703fabb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45703fabb0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f45703faa60>\n",
      "args = (NDArray([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0.64388...2  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]], device=cpu()))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[ 0.23885646  1.7992946   0.17290778 -0.18845753  1.6103987 ]\n",
      " [-0.57978904 -1.7727456   0.7769845  -0.64388...2  -1.3194236  -2.0160983   0.74200845]\n",
      " [-1.2923242   0.1273649  -1.225224    1.421074    1.0188165 ]], device=cpu()))\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f45703faa60>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928 ... 1.0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])]\n",
      "A_t        = [tensor([[[-1.3506, -1.0219, -0.2898, -0.5111, -1.0806, -0.1566, -0.1900],\n",
      "         [-1.5699,  0.1507, -1.5068,  0.479...2785,  0.1121, -0.2383,  1.0998, -0.0714],\n",
      "         [ 0.6218,  0.6492,  1.0329, -0.8242, -0.0909,  0.3844,  0.3634]]])]\n",
      "_A         = [array([[[-1.3505836 , -1.0219276 , -0.28979254, -0.5110697 ,\n",
      "         -1.0806233 , -0.156554  , -0.18995644],\n",
      "       ...[ 0.62180406,  0.64916927,  1.0328673 , -0.82423985,\n",
      "         -0.09091089,  0.3844313 ,  0.36341754]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928 ... 1.0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.1899....0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f4571323c10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.1899....0998238  -0.07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]])),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f4571323c10>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713234f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713234f0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f4571323c10>\n",
      "args = (NDArray([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928    0.1....07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]], device=cpu()))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-1.3505836  -1.0219276  -0.28979254 -0.5110697  -1.0806233\n",
      "   -0.156554   -0.18995644]\n",
      "  [-1.569928    0.1....07136466]\n",
      "  [ 0.62180406  0.64916927  1.0328673  -0.82423985 -0.09091089\n",
      "    0.3844313   0.36341754]]], device=cpu()))\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f4571323c10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1...948   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]])]\n",
      "A_t        = [tensor([[-0.8576, -0.0482,  0.2440,  1.8527,  0.0825],\n",
      "        [-1.3493,  0.7654, -1.0452,  1.2154, -1.8788],\n",
      "       ....7364],\n",
      "        [ 0.6130,  0.6366, -0.1788,  1.4526,  0.0686],\n",
      "        [-0.0181, -0.1486, -1.0339, -1.0570, -0.2119]])]\n",
      "_A         = [array([[-0.85764205, -0.04821775,  0.24398687,  1.8527063 ,  0.08247776],\n",
      "       [-1.3493153 ,  0.7653674 , -1.045237...526075 ,  0.06863618],\n",
      "       [-0.0180711 , -0.14859328, -1.0338513 , -1.0570263 , -0.2118788 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1...948   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653...   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]]),),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f4570476730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653...   0.6366247  -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]]),),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f4570476730>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f4570476910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f4570476910>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f4570476730>\n",
      "args = (NDArray([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1.21536... -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]], device=cuda()),)\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-0.85764205 -0.04821775  0.24398687  1.8527063   0.08247776]\n",
      " [-1.3493153   0.7653674  -1.0452379   1.21536... -0.17882441  1.4526075   0.06863618]\n",
      " [-0.0180711  -0.14859328 -1.0338513  -1.0570263  -0.2118788 ]], device=cuda()),)\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f4570476730>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0...7274  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])]\n",
      "A_t        = [tensor([[-0.0361,  1.2204,  0.7632, -0.0202,  2.2113],\n",
      "        [-0.4971,  1.6844, -0.6964,  0.4664,  0.3781],\n",
      "       ....2980],\n",
      "        [ 0.0499,  1.2440, -1.3311,  0.1839, -0.7144],\n",
      "        [-1.1293, -1.0743,  0.6784,  0.4646,  1.6899]])]\n",
      "_A         = [array([[-0.03613054,  1.2204468 ,  0.7631518 , -0.02020102,  2.2112603 ],\n",
      "       [-0.49705917,  1.6843735 , -0.696395...8387789, -0.7143793 ],\n",
      "       [-1.1292638 , -1.0743438 ,  0.6784423 ,  0.46458447,  1.6899037 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0...7274  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843...74  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f45712f8700>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843...74  1.2440314  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]])),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f45712f8700>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712f8340>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712f8340>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f45712f8700>\n",
      "args = (NDArray([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0.46635...  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]], device=cuda()))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-0.03613054  1.2204468   0.7631518  -0.02020102  2.2112603 ]\n",
      " [-0.49705917  1.6843735  -0.6963952   0.46635...  -1.3311231   0.18387789 -0.7143793 ]\n",
      " [-1.1292638  -1.0743438   0.6784423   0.46458447  1.6899037 ]], device=cuda()))\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f45712f8700>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      ">       out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  ...02]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])]\n",
      "A_t        = [tensor([[[-5.5682e-01, -8.2674e-01, -9.5333e-01, -1.0309e-01, -4.2893e-01,\n",
      "           5.5746e-04,  1.0586e+00],\n",
      "     ...02],\n",
      "         [ 1.1807e+00, -5.7011e-01, -1.6425e-01,  1.0571e-01, -1.1592e-02,\n",
      "           6.3662e-02,  3.1759e-01]]])]\n",
      "_A         = [array([[[-5.56821287e-01, -8.26741934e-01, -9.53326285e-01,\n",
      "         -1.03087865e-01, -4.28928941e-01,  5.57456922e-0...011104e-01, -1.6424966e-01,  1.0570560e-01,\n",
      "         -1.1592215e-02,  6.3661553e-02,  3.1758705e-01]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  ...02]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-0...]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f45713bfcd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-0...]\n",
      "  [ 1.1806753e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]])),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f45713bfcd0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713bfc10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713bfc10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f45713bfcd0>\n",
      "args = (NDArray([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  1.0586...53e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]], device=cuda()))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-5.56821287e-01 -8.26741934e-01 -9.53326285e-01 -1.03087865e-01\n",
      "   -4.28928941e-01  5.57456922e-04  1.0586...53e+00 -5.7011104e-01 -1.6424966e-01  1.0570560e-01\n",
      "   -1.1592215e-02  6.3661553e-02  3.1758705e-01]]], device=cuda()))\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f45713bfcd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape0-0-1] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0...1785 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]])]\n",
      "A_t        = [tensor([[-0.9027, -0.7916,  2.7447,  1.3618, -0.8795],\n",
      "        [-0.8366, -1.3726,  0.8278, -0.1998, -1.3305],\n",
      "       ...0961, -0.1999, -0.4484,  1.0540,  0.0305],\n",
      "        [-0.4769,  1.5635, -1.0746,  0.9731, -1.0777]], requires_grad=True)]\n",
      "_A         = [array([[-0.9026891 , -0.7916189 ,  2.7447178 ,  1.3617536 , -0.8795233 ],\n",
      "       [-0.8365751 , -1.3725866 ,  0.827849...540462 ,  0.03045906],\n",
      "       [-0.4768823 ,  1.5634812 , -1.0746397 ,  0.9731278 , -1.0777118 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0...1785 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725...5 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]]),),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f45712d56d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725...5 -0.1998671  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]]),),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f45712d56d0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712d5d60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712d5d60>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f45712d56d0>\n",
      "args = (NDArray([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0.19975...  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]], device=cpu()),)\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-0.9026891  -0.7916189   2.7447178   1.3617536  -0.8795233 ]\n",
      " [-0.8365751  -1.3725866   0.82784927 -0.19975...  -0.4484034   1.0540462   0.03045906]\n",
      " [-0.4768823   1.5634812  -1.0746397   0.9731278  -1.0777118 ]], device=cpu()),)\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f45712d56d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape1-0-2] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0...7174  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])]\n",
      "A_t        = [tensor([[-1.1209, -0.4109, -0.1834,  1.0602,  0.6528],\n",
      "        [ 0.5972,  0.9693,  1.1120, -0.5864,  0.0547],\n",
      "       ...3290,  0.2979,  0.3833,  0.1805,  0.1305],\n",
      "        [ 0.8502,  0.6713,  1.1949, -0.3069, -0.4117]], requires_grad=True)]\n",
      "_A         = [array([[-1.1208949 , -0.41087958, -0.18338212,  1.0601658 ,  0.6527857 ],\n",
      "       [ 0.5972073 ,  0.9692809 ,  1.112041...8051435,  0.1305337 ],\n",
      "       [ 0.8501626 ,  0.6712771 ,  1.1948769 , -0.30687174, -0.41171578]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0...7174  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692...74  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f45712af580>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692...74  0.29785007  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]])),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f45712af580>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712aff10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712aff10>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f45712af580>\n",
      "args = (NDArray([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0.58642...07  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]], device=cpu()))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-1.1208949  -0.41087958 -0.18338212  1.0601658   0.6527857 ]\n",
      " [ 0.5972073   0.9692809   1.112041   -0.58642...07  0.38333625  0.18051435  0.1305337 ]\n",
      " [ 0.8501626   0.6712771   1.1948769  -0.30687174 -0.41171578]], device=cpu()))\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f45712af580>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape2-2-5] ______________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.2332015...-1.0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])]\n",
      "A_t        = [tensor([[[-0.2897,  0.5881,  1.2632,  1.8495, -1.3362, -0.2293,  1.6220],\n",
      "         [ 0.2332,  0.4612, -1.1698,  1.140...0655, -0.2649],\n",
      "         [-2.1368, -0.2813,  0.5420, -0.8280, -0.1952,  1.2885,  1.6259]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[-0.28966966,  0.5881318 ,  1.2632097 ,  1.8494787 ,\n",
      "         -1.3362346 , -0.22933096,  1.6220489 ],\n",
      "       ...[-2.136809  , -0.28127787,  0.54197794, -0.8279596 ,\n",
      "         -0.19517688,  1.2884692 ,  1.625865  ]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.2332015...-1.0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220....0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f45703e24f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220....0654556  -0.26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]])),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f45703e24f0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45703e2340>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45703e2340>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f45703e24f0>\n",
      "args = (NDArray([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.23320156  0.4....26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]], device=cpu()))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-0.28966966  0.5881318   1.2632097   1.8494787  -1.3362346\n",
      "   -0.22933096  1.6220489 ]\n",
      "  [ 0.23320156  0.4....26491052]\n",
      "  [-2.136809   -0.28127787  0.54197794 -0.8279596  -0.19517688\n",
      "    1.2884692   1.625865  ]]], device=cpu()))\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f45703e24f0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape0-0-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0...197  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]])]\n",
      "A_t        = [tensor([[-1.3193,  0.6103, -1.2339, -0.6026,  0.8272],\n",
      "        [ 1.3493,  0.1192, -0.3294,  0.6092,  1.7408],\n",
      "       ...5734, -0.6012, -0.2230,  0.0861,  0.1369],\n",
      "        [ 1.5095, -0.7127, -0.7338, -1.1747, -0.3218]], requires_grad=True)]\n",
      "_A         = [array([[-1.3193388 ,  0.6103235 , -1.2338529 , -0.6025919 ,  0.82718664],\n",
      "       [ 1.349268  ,  0.11915252, -0.329409...8610313,  0.13694113],\n",
      "       [ 1.5095038 , -0.7126971 , -0.73380625, -1.1747231 , -0.3217507 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0...197  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.1191...  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]]),),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f45712f3ac0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.1191...  -0.6012362  -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]]),),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f45712f3ac0>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712f3430>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45712f3430>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f45712f3ac0>\n",
      "args = (NDArray([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0.60915... -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]], device=cuda()),)\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[-1.3193388   0.6103235  -1.2338529  -0.6025919   0.82718664]\n",
      " [ 1.349268    0.11915252 -0.32940993  0.60915... -0.2229816   0.08610313  0.13694113]\n",
      " [ 1.5095038  -0.7126971  -0.73380625 -1.1747231  -0.3217507 ]], device=cuda()),)\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f45712f3ac0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape1-0-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2...63    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])]\n",
      "A_t        = [tensor([[ 0.8628, -1.1587, -1.0618,  0.0683,  0.4480],\n",
      "        [-1.6560,  0.3594,  0.5834, -2.7582,  0.1567],\n",
      "       ...2584,  0.9400,  1.4049,  2.0662,  1.9755],\n",
      "        [ 0.7066, -1.1856, -1.0203, -0.1288, -0.1320]], requires_grad=True)]\n",
      "_A         = [array([[ 0.86278856, -1.1587424 , -1.0617669 ,  0.06834911,  0.44795617],\n",
      "       [-1.6559892 ,  0.35942334,  0.583443...662467 ,  1.975455  ],\n",
      "       [ 0.70656043, -1.1856185 , -1.0202792 , -0.12881304, -0.13199753]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2...63    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.3594...    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f45713ced00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.3594...    0.94000864  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]])),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f45713ced00>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713cefd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f45713cefd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f45713ced00>\n",
      "args = (NDArray([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2.75821...4  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]], device=cuda()))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[ 0.86278856 -1.1587424  -1.0617669   0.06834911  0.44795617]\n",
      " [-1.6559892   0.35942334  0.5834437  -2.75821...4  1.4048624   2.0662467   1.975455  ]\n",
      " [ 0.70656043 -1.1856185  -1.0202792  -0.12881304 -0.13199753]], device=cuda()))\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f45713ced00>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape2-2-5] _____________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = [needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232 ...    0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])]\n",
      "A_t        = [tensor([[[-0.5643,  0.2627, -0.2986,  2.1484,  2.2822,  2.0546, -0.5192],\n",
      "         [ 0.5852,  0.2980, -0.4843, -0.240...2493, -1.1226],\n",
      "         [ 1.3219, -0.1445, -0.9866,  0.2616,  0.9347, -0.3751,  0.2546]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[-0.56430817,  0.26270753, -0.29860306,  2.1484196 ,\n",
      "          2.282201  ,  2.0546498 , -0.51918304],\n",
      "       ...[ 1.3219295 , -0.14445436, -0.9865747 ,  0.2616311 ,\n",
      "          0.93468   , -0.37505797,  0.25460657]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:379: in stack\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\u001b[90m\u001b[39;49;00m\n",
      "        args       = [needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232 ...    0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918...  0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])),)\n",
      "        self       = <needle.ops.ops_mathematic.Stack object at 0x7f457045a820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918...  0.24931994 -1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]])),)\n",
      "        op         = <needle.ops.ops_mathematic.Stack object at 0x7f457045a820>\n",
      "        tensor     = <[NotImplementedError() raised in repr()] Tensor object at 0x7f457045aa90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[NotImplementedError() raised in repr()] Tensor object at 0x7f457045aa90>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Stack object at 0x7f457045a820>\n",
      "args = (NDArray([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232   0.29...-1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]], device=cuda()))\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, args: TensorTuple) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "args       = (NDArray([[[-0.56430817  0.26270753 -0.29860306  2.1484196   2.282201\n",
      "    2.0546498  -0.51918304]\n",
      "  [ 0.5852232   0.29...-1.122568  ]\n",
      "  [ 1.3219295  -0.14445436 -0.9865747   0.2616311   0.93468\n",
      "   -0.37505797  0.25460657]]], device=cuda()))\n",
      "self       = <needle.ops.ops_mathematic.Stack object at 0x7f457045a820>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:368: NotImplementedError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape1-0] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 0.36832857  0.524743    0.72734344]\n",
      " [-0.6690794   0.2118702  -0.17624503]\n",
      " [-0.5157768  -1.9483194   0.21689712]\n",
      " [ 0.43637756 -0.49504334  1.4166641 ]\n",
      " [-0.26324236  1.5031598  -1.2881242 ]])\n",
      "_A         = array([[ 0.36832854,  0.524743  ,  0.72734344],\n",
      "       [-0.6690794 ,  0.2118702 , -0.17624503],\n",
      "       [-0.5157768 , -...89712],\n",
      "       [ 0.43637753, -0.49504337,  1.4166641 ],\n",
      "       [-0.2632424 ,  1.5031598 , -1.2881242 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 0.36832857  0.524743    0.72734344]\n",
      " [-0.6690794   0.2118702  -0.17624503]\n",
      " [-0.5157768  -1.9483194   0.21689712]\n",
      " [ 0.43637756 -0.49504334  1.4166641 ]\n",
      " [-0.26324236  1.5031598  -1.2881242 ]]),)\n",
      "        c          = array([-0.24279231,  0.51338752,  1.17676622])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f460fbab310>\n",
      "        f1         = 1.1067148318683209\n",
      "        f2         = 1.1066912645822016\n",
      "        i          = 0\n",
      "        j          = 14\n",
      "        kwargs     = {'axes': 0}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[-0.24312203,  0.51408472,  1.17836431],\n",
      "       [-0.24312203,  0.51408472,  1.17836431],\n",
      "       [-0.24312203, ...408472,  1.16433616],\n",
      "       [-0.24239845,  0.51102469,  1.17836431],\n",
      "       [-0.24239845,  0.51408472,  1.17836431]])]\n",
      "        out        = needle.Tensor([-0.64339256 -0.2035898   0.8965355 ])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([-0.64339256 -0.2035898   0.8965355 ])\n",
      "        out_grad   = needle.Tensor([-0.24279231  0.5133875   1.1767663 ])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f45712affd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f45712affd0>\n",
      "out_grad = needle.Tensor([-0.24279231  0.5133875   1.1767663 ])\n",
      "node = needle.Tensor([-0.64339256 -0.2035898   0.8965355 ])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        new_shape = \u001b[96mlist\u001b[39;49;00m(node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "        axes = \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(new_shape)) \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# add ones to reduced idx for broadcast\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mfor\u001b[39;49;00m axis \u001b[95min\u001b[39;49;00m axes:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "axes       = 0\n",
      "new_shape  = [5, 3]\n",
      "node       = needle.Tensor([-0.64339256 -0.2035898   0.8965355 ])\n",
      "out_grad   = needle.Tensor([-0.24279231  0.5133875   1.1767663 ])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f45712affd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:241: TypeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape2-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.223357    1.9838535 ]\n",
      "  [-0.40351704  1.5878327 ]\n",
      "  [ 1.8947744   1.2847524 ]]\n",
      "\n",
      " [[-0.955546   -0....3]\n",
      "  [ 0.66463155  0.21836282]]\n",
      "\n",
      " [[ 0.6723505  -0.5805451 ]\n",
      "  [ 0.94992834 -0.13286018]\n",
      "  [-0.05775289  2.3362038 ]]])\n",
      "_A         = array([[[-1.223357  ,  1.9838535 ],\n",
      "        [-0.40351707,  1.5878327 ],\n",
      "        [ 1.8947744 ,  1.2847524 ]],\n",
      "\n",
      "       [...  [[ 0.6723505 , -0.5805451 ],\n",
      "        [ 0.94992834, -0.13286018],\n",
      "        [-0.05775288,  2.3362038 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.223357    1.9838535 ]\n",
      "  [-0.40351704  1.5878327 ]\n",
      "  [ 1.8947744   1.2847524 ]]\n",
      "\n",
      " [[-0.955546   -0...\n",
      "  [ 0.66463155  0.21836282]]\n",
      "\n",
      " [[ 0.6723505  -0.5805451 ]\n",
      "  [ 0.94992834 -0.13286018]\n",
      "  [-0.05775289  2.3362038 ]]]),)\n",
      "        c          = array([[-0.02063232,  0.17829331],\n",
      "       [ 1.29770819,  1.22375498],\n",
      "       [ 0.56194033,  0.57480341],\n",
      "       [-0.39...-0.5583447 ],\n",
      "       [-0.85796767,  0.14308997],\n",
      "       [-1.59867454, -3.0235811 ],\n",
      "       [ 0.22697888, -1.57959625]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f460fbab310>\n",
      "        f1         = -4.059700185351925\n",
      "        f2         = -4.0596685505241386\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 1}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-0.02066034,  0.17853544],\n",
      "        [-0.02066034,  0.17853544],\n",
      "        [-0.02066034,  0.17853544]],\n",
      "\n",
      "       ...7625]],\n",
      "\n",
      "       [[ 0.22728712, -1.58174139],\n",
      "        [ 0.22728712, -1.58174139],\n",
      "        [ 0.22728712, -1.58174139]]])]\n",
      "        out        = needle.Tensor([[ 0.26790047  4.8564386 ]\n",
      " [-0.68675244 -1.1846567 ]\n",
      " [ 0.08269829  1.9032902 ]\n",
      " [ 0.05148055 -0.08501023]\n",
      " [ 1.3483511   2.799955  ]\n",
      " [ 0.762841    0.44975713]\n",
      " [-0.68042046 -0.5701694 ]\n",
      " [ 1.5645261   1.6227984 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[ 0.26790047  4.8564386 ]\n",
      " [-0.68675244 -1.1846567 ]\n",
      " [ 0.08269829  1.9032902 ]\n",
      " [ 0.05148055 -0.08501023]\n",
      " [ 1.3483511   2.799955  ]\n",
      " [ 0.762841    0.44975713]\n",
      " [-0.68042046 -0.5701694 ]\n",
      " [ 1.5645261   1.6227984 ]])\n",
      "        out_grad   = needle.Tensor([[-0.02063232  0.17829332]\n",
      " [ 1.2977082   1.223755  ]\n",
      " [ 0.5619403   0.5748034 ]\n",
      " [-0.39962965  0.39787385]\n",
      " [-1.5687171  -0.5583447 ]\n",
      " [-0.8579677   0.14308996]\n",
      " [-1.5986745  -3.023581  ]\n",
      " [ 0.22697888 -1.5795963 ]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f462ec81d30>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f462ec81d30>\n",
      "out_grad = needle.Tensor([[-0.02063232  0.17829332]\n",
      " [ 1.2977082   1.223755  ]\n",
      " [ 0.5619403   0.5748034 ]\n",
      " [-0.39962965  0.39787385]\n",
      " [-1.5687171  -0.5583447 ]\n",
      " [-0.8579677   0.14308996]\n",
      " [-1.5986745  -3.023581  ]\n",
      " [ 0.22697888 -1.5795963 ]])\n",
      "node = needle.Tensor([[ 0.26790047  4.8564386 ]\n",
      " [-0.68675244 -1.1846567 ]\n",
      " [ 0.08269829  1.9032902 ]\n",
      " [ 0.05148055 -0.08501023]\n",
      " [ 1.3483511   2.799955  ]\n",
      " [ 0.762841    0.44975713]\n",
      " [-0.68042046 -0.5701694 ]\n",
      " [ 1.5645261   1.6227984 ]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        new_shape = \u001b[96mlist\u001b[39;49;00m(node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "        axes = \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(new_shape)) \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# add ones to reduced idx for broadcast\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mfor\u001b[39;49;00m axis \u001b[95min\u001b[39;49;00m axes:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "axes       = 1\n",
      "new_shape  = [8, 3, 2]\n",
      "node       = needle.Tensor([[ 0.26790047  4.8564386 ]\n",
      " [-0.68675244 -1.1846567 ]\n",
      " [ 0.08269829  1.9032902 ]\n",
      " [ 0.05148055 -0.08501023]\n",
      " [ 1.3483511   2.799955  ]\n",
      " [ 0.762841    0.44975713]\n",
      " [-0.68042046 -0.5701694 ]\n",
      " [ 1.5645261   1.6227984 ]])\n",
      "out_grad   = needle.Tensor([[-0.02063232  0.17829332]\n",
      " [ 1.2977082   1.223755  ]\n",
      " [ 0.5619403   0.5748034 ]\n",
      " [-0.39962965  0.39787385]\n",
      " [-1.5687171  -0.5583447 ]\n",
      " [-0.8579677   0.14308996]\n",
      " [-1.5986745  -3.023581  ]\n",
      " [ 0.22697888 -1.5795963 ]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f462ec81d30>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:241: TypeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape3-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 0.00771675 -0.477715  ]\n",
      "  [-0.37093356 -1.6240172 ]\n",
      "  [-1.6641127   0.5510251 ]]\n",
      "\n",
      " [[-0.24003424 -0....6]\n",
      "  [ 0.06870755 -1.2057985 ]]\n",
      "\n",
      " [[-1.0558244   1.0220021 ]\n",
      "  [-0.20333268  0.9668824 ]\n",
      "  [-0.25639063  0.5157999 ]]])\n",
      "_A         = array([[[ 0.00771675, -0.47771502],\n",
      "        [-0.3709336 , -1.6240172 ],\n",
      "        [-1.6641127 ,  0.5510251 ]],\n",
      "\n",
      "       [...  [[-1.0558244 ,  1.0220021 ],\n",
      "        [-0.20333268,  0.9668824 ],\n",
      "        [-0.25639066,  0.5157999 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 0.00771675 -0.477715  ]\n",
      "  [-0.37093356 -1.6240172 ]\n",
      "  [-1.6641127   0.5510251 ]]\n",
      "\n",
      " [[-0.24003424 -0...\n",
      "  [ 0.06870755 -1.2057985 ]]\n",
      "\n",
      " [[-1.0558244   1.0220021 ]\n",
      "  [-0.20333268  0.9668824 ]\n",
      "  [-0.25639063  0.5157999 ]]]),)\n",
      "        c          = array([[-0.44359782,  0.83550506, -0.75412982],\n",
      "       [-1.34921992,  0.28219448,  0.24463519],\n",
      "       [ 0.36400867,  ...8540656,  1.06528525],\n",
      "       [ 1.86921905, -0.38329579,  0.27533789],\n",
      "       [-0.95338377,  0.44119945, -0.18395162]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f460fbab310>\n",
      "        f1         = 0.42565076588967754\n",
      "        f2         = 0.42565444991823\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 2}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-0.44353923, -0.44353923],\n",
      "        [ 0.8366397 ,  0.8366397 ],\n",
      "        [-0.75515395, -0.75515395]],\n",
      "\n",
      "       ...1181]],\n",
      "\n",
      "       [[-0.9546785 , -0.9546785 ],\n",
      "        [ 0.44048374,  0.44179862],\n",
      "        [-0.18392732, -0.18420143]]])]\n",
      "        out        = needle.Tensor([[-0.46999827 -1.9949508  -1.1130877 ]\n",
      " [-1.2167768  -0.14212924 -2.7648933 ]\n",
      " [-0.8610341   1.371288   ...\n",
      " [-0.2699802   0.9004525  -0.72803926]\n",
      " [ 1.6400697   2.2829142  -1.1370909 ]\n",
      " [-0.0338223   0.76354975  0.25940922]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[-0.46999827 -1.9949508  -1.1130877 ]\n",
      " [-1.2167768  -0.14212924 -2.7648933 ]\n",
      " [-0.8610341   1.371288   ...\n",
      " [-0.2699802   0.9004525  -0.72803926]\n",
      " [ 1.6400697   2.2829142  -1.1370909 ]\n",
      " [-0.0338223   0.76354975  0.25940922]])\n",
      "        out_grad   = needle.Tensor([[-0.44359782  0.83550507 -0.7541298 ]\n",
      " [-1.3492199   0.28219447  0.2446352 ]\n",
      " [ 0.36400867  0.6355233  ...\n",
      " [-1.0691541  -1.2854066   1.0652852 ]\n",
      " [ 1.8692191  -0.38329577  0.27533787]\n",
      " [-0.95338374  0.44119945 -0.18395162]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f45713a95e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f45713a95e0>\n",
      "out_grad = needle.Tensor([[-0.44359782  0.83550507 -0.7541298 ]\n",
      " [-1.3492199   0.28219447  0.2446352 ]\n",
      " [ 0.36400867  0.6355233  ...\n",
      " [-1.0691541  -1.2854066   1.0652852 ]\n",
      " [ 1.8692191  -0.38329577  0.27533787]\n",
      " [-0.95338374  0.44119945 -0.18395162]])\n",
      "node = needle.Tensor([[-0.46999827 -1.9949508  -1.1130877 ]\n",
      " [-1.2167768  -0.14212924 -2.7648933 ]\n",
      " [-0.8610341   1.371288   ...\n",
      " [-0.2699802   0.9004525  -0.72803926]\n",
      " [ 1.6400697   2.2829142  -1.1370909 ]\n",
      " [-0.0338223   0.76354975  0.25940922]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        new_shape = \u001b[96mlist\u001b[39;49;00m(node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "        axes = \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(new_shape)) \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# add ones to reduced idx for broadcast\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mfor\u001b[39;49;00m axis \u001b[95min\u001b[39;49;00m axes:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "axes       = 2\n",
      "new_shape  = [8, 3, 2]\n",
      "node       = needle.Tensor([[-0.46999827 -1.9949508  -1.1130877 ]\n",
      " [-1.2167768  -0.14212924 -2.7648933 ]\n",
      " [-0.8610341   1.371288   ...\n",
      " [-0.2699802   0.9004525  -0.72803926]\n",
      " [ 1.6400697   2.2829142  -1.1370909 ]\n",
      " [-0.0338223   0.76354975  0.25940922]])\n",
      "out_grad   = needle.Tensor([[-0.44359782  0.83550507 -0.7541298 ]\n",
      " [-1.3492199   0.28219447  0.2446352 ]\n",
      " [ 0.36400867  0.6355233  ...\n",
      " [-1.0691541  -1.2854066   1.0652852 ]\n",
      " [ 1.8692191  -0.38329577  0.27533787]\n",
      " [-0.95338374  0.44119945 -0.18395162]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f45713a95e0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:241: TypeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape1-0] ____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[-0.40711153 -0.80170184 -1.1139419 ]\n",
      " [ 0.3914071  -1.674953   -0.23902948]\n",
      " [ 0.7553679  -1.4470354   1.9484388 ]\n",
      " [ 0.5264981  -0.20674394  1.478983  ]\n",
      " [ 0.37850133 -1.1597295   0.8766111 ]])\n",
      "_A         = array([[-0.40711156, -0.80170184, -1.1139419 ],\n",
      "       [ 0.39140707, -1.674953  , -0.23902948],\n",
      "       [ 0.7553679 , -...4388 ],\n",
      "       [ 0.5264981 , -0.20674394,  1.478983  ],\n",
      "       [ 0.3785013 , -1.1597295 ,  0.8766111 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[-0.40711153 -0.80170184 -1.1139419 ]\n",
      " [ 0.3914071  -1.674953   -0.23902948]\n",
      " [ 0.7553679  -1.4470354   1.9484388 ]\n",
      " [ 0.5264981  -0.20674394  1.478983  ]\n",
      " [ 0.37850133 -1.1597295   0.8766111 ]]),)\n",
      "        c          = array([-1.75722884,  0.31880177, -1.65264358])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f460fbab310>\n",
      "        f1         = -9.45363223369758\n",
      "        f2         = -9.453599135939179\n",
      "        i          = 0\n",
      "        j          = 14\n",
      "        kwargs     = {'axes': 0}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[-1.75961521,  0.31923471, -1.65488792],\n",
      "       [-1.74914131,  0.31923471, -1.65488792],\n",
      "       [-1.75961521, ...923471, -1.65488792],\n",
      "       [-1.75961521,  0.31163388, -1.65488792],\n",
      "       [-1.75961521,  0.31923471, -1.65488792]])]\n",
      "        out        = needle.Tensor([ 1.6446627 -5.2901635  2.9510617])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([ 1.6446627 -5.2901635  2.9510617])\n",
      "        out_grad   = needle.Tensor([-1.7572289   0.31880176 -1.6526436 ])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f4571393700>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f4571393700>\n",
      "out_grad = needle.Tensor([-1.7572289   0.31880176 -1.6526436 ])\n",
      "node = needle.Tensor([ 1.6446627 -5.2901635  2.9510617])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        new_shape = \u001b[96mlist\u001b[39;49;00m(node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "        axes = \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(new_shape)) \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# add ones to reduced idx for broadcast\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mfor\u001b[39;49;00m axis \u001b[95min\u001b[39;49;00m axes:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "axes       = 0\n",
      "new_shape  = [5, 3]\n",
      "node       = needle.Tensor([ 1.6446627 -5.2901635  2.9510617])\n",
      "out_grad   = needle.Tensor([-1.7572289   0.31880176 -1.6526436 ])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f4571393700>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:241: TypeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape2-1] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 3.1887996e-01  1.5159521e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "\n",
      "...2054883e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395303e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]])\n",
      "_A         = array([[[ 3.1887993e-01,  1.5159522e-03],\n",
      "        [-9.1810602e-01,  8.5386848e-01],\n",
      "        [ 5.3351974e-01,  6.397005...,  3.2395300e-01],\n",
      "        [ 2.3655061e-01,  2.1437683e+00],\n",
      "        [ 1.1814048e+00, -1.3010720e+00]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 3.1887996e-01  1.5159521e-03]\n",
      "  [-9.1810602e-01  8.5386848e-01]\n",
      "  [ 5.3351974e-01  6.3970059e-01]]\n",
      "...54883e-01]]\n",
      "\n",
      " [[ 1.4263909e+00  3.2395303e-01]\n",
      "  [ 2.3655061e-01  2.1437683e+00]\n",
      "  [ 1.1814048e+00 -1.3010720e+00]]]),)\n",
      "        c          = array([[-0.43455654,  0.55048798],\n",
      "       [ 0.06143115,  1.20697315],\n",
      "       [ 0.25282092, -0.36453617],\n",
      "       [-0.16...-1.43825386],\n",
      "       [ 0.05111475,  1.69073882],\n",
      "       [ 1.66974421, -1.40444807],\n",
      "       [-1.4388192 , -0.02291146]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f460fbab310>\n",
      "        f1         = 16.73120485004283\n",
      "        f2         = 16.73120530889439\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 1}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-0.43385161,  0.55123556],\n",
      "        [-0.43514669,  0.55123556],\n",
      "        [-0.43514669,  0.55123556]],\n",
      "\n",
      "       ...6256]],\n",
      "\n",
      "       [[-1.44077317, -0.02266945],\n",
      "        [-1.44077317, -0.02294258],\n",
      "        [-1.44077317, -0.02294258]]])]\n",
      "        out        = needle.Tensor([[-0.06570637  1.495085  ]\n",
      " [-0.77701896 -0.59986424]\n",
      " [ 0.78010964 -2.325438  ]\n",
      " [-0.16809821  3.5168025 ]\n",
      " [ 1.8090553  -2.381361  ]\n",
      " [ 0.66218865  2.0200748 ]\n",
      " [ 1.4608432   0.38579413]\n",
      " [ 2.8443463   1.1666492 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[-0.06570637  1.495085  ]\n",
      " [-0.77701896 -0.59986424]\n",
      " [ 0.78010964 -2.325438  ]\n",
      " [-0.16809821  3.5168025 ]\n",
      " [ 1.8090553  -2.381361  ]\n",
      " [ 0.66218865  2.0200748 ]\n",
      " [ 1.4608432   0.38579413]\n",
      " [ 2.8443463   1.1666492 ]])\n",
      "        out_grad   = needle.Tensor([[-0.43455654  0.550488  ]\n",
      " [ 0.06143114  1.2069732 ]\n",
      " [ 0.2528209  -0.36453617]\n",
      " [-0.1625579   2.2881606 ]\n",
      " [ 1.5918057  -1.4382539 ]\n",
      " [ 0.05111475  1.6907388 ]\n",
      " [ 1.6697443  -1.404448  ]\n",
      " [-1.4388192  -0.02291146]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f45703e28e0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f45703e28e0>\n",
      "out_grad = needle.Tensor([[-0.43455654  0.550488  ]\n",
      " [ 0.06143114  1.2069732 ]\n",
      " [ 0.2528209  -0.36453617]\n",
      " [-0.1625579   2.2881606 ]\n",
      " [ 1.5918057  -1.4382539 ]\n",
      " [ 0.05111475  1.6907388 ]\n",
      " [ 1.6697443  -1.404448  ]\n",
      " [-1.4388192  -0.02291146]])\n",
      "node = needle.Tensor([[-0.06570637  1.495085  ]\n",
      " [-0.77701896 -0.59986424]\n",
      " [ 0.78010964 -2.325438  ]\n",
      " [-0.16809821  3.5168025 ]\n",
      " [ 1.8090553  -2.381361  ]\n",
      " [ 0.66218865  2.0200748 ]\n",
      " [ 1.4608432   0.38579413]\n",
      " [ 2.8443463   1.1666492 ]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        new_shape = \u001b[96mlist\u001b[39;49;00m(node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "        axes = \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(new_shape)) \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# add ones to reduced idx for broadcast\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mfor\u001b[39;49;00m axis \u001b[95min\u001b[39;49;00m axes:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "axes       = 1\n",
      "new_shape  = [8, 3, 2]\n",
      "node       = needle.Tensor([[-0.06570637  1.495085  ]\n",
      " [-0.77701896 -0.59986424]\n",
      " [ 0.78010964 -2.325438  ]\n",
      " [-0.16809821  3.5168025 ]\n",
      " [ 1.8090553  -2.381361  ]\n",
      " [ 0.66218865  2.0200748 ]\n",
      " [ 1.4608432   0.38579413]\n",
      " [ 2.8443463   1.1666492 ]])\n",
      "out_grad   = needle.Tensor([[-0.43455654  0.550488  ]\n",
      " [ 0.06143114  1.2069732 ]\n",
      " [ 0.2528209  -0.36453617]\n",
      " [-0.1625579   2.2881606 ]\n",
      " [ 1.5918057  -1.4382539 ]\n",
      " [ 0.05111475  1.6907388 ]\n",
      " [ 1.6697443  -1.404448  ]\n",
      " [-1.4388192  -0.02291146]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f45703e28e0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:241: TypeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape3-2] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 0.04162927 -1.2710459 ]\n",
      "  [ 0.00565341  0.68353325]\n",
      "  [-1.1112776  -2.5426693 ]]\n",
      "\n",
      " [[ 0.51752055  0.... ]\n",
      "  [-0.25414678  1.4692454 ]]\n",
      "\n",
      " [[-0.25943646  1.2197713 ]\n",
      "  [-0.9723478  -0.6831798 ]\n",
      "  [ 0.37447047  0.16003066]]])\n",
      "_A         = array([[[ 0.04162927, -1.2710459 ],\n",
      "        [ 0.00565341,  0.68353325],\n",
      "        [-1.1112776 , -2.5426693 ]],\n",
      "\n",
      "       [...  [[-0.2594365 ,  1.2197713 ],\n",
      "        [-0.9723478 , -0.6831798 ],\n",
      "        [ 0.37447044,  0.16003066]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    \u001b[0mbackward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 0.04162927 -1.2710459 ]\n",
      "  [ 0.00565341  0.68353325]\n",
      "  [-1.1112776  -2.5426693 ]]\n",
      "\n",
      " [[ 0.51752055  0...\n",
      "  [-0.25414678  1.4692454 ]]\n",
      "\n",
      " [[-0.25943646  1.2197713 ]\n",
      "  [-0.9723478  -0.6831798 ]\n",
      "  [ 0.37447047  0.16003066]]]),)\n",
      "        c          = array([[ 1.27281975,  1.10007122, -0.74093072],\n",
      "       [-0.50888016,  0.40810799,  0.53634965],\n",
      "       [-2.11181617, -...5634464, -0.22966216],\n",
      "       [-1.2457664 ,  0.18546481, -0.2667139 ],\n",
      "       [-0.31800157, -1.28237754,  1.4503455 ]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7f460fbab310>\n",
      "        f1         = -3.5786765085401635\n",
      "        f2         = -3.578705554842454\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 2}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[ 1.27454828,  1.27454828],\n",
      "        [ 1.09828668,  1.10156515],\n",
      "        [-0.74193693, -0.74193693]],\n",
      "\n",
      "       ...761 ]],\n",
      "\n",
      "       [[-0.31843342, -0.31843342],\n",
      "        [-1.28411905, -1.28411905],\n",
      "        [ 1.44799275,  1.45231511]]])]\n",
      "        out        = needle.Tensor([[-1.2294166   0.68918663 -3.6539469 ]\n",
      " [ 0.98761314 -0.05946481 -0.21467207]\n",
      " [ 1.6573914   0.5526084  ...\n",
      " [ 3.1226218   1.3280458  -0.49049693]\n",
      " [ 0.6714237   1.1756382   1.2150986 ]\n",
      " [ 0.9603348  -1.6555276   0.5345011 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:67: in gradient_as_tuple\n",
      "    \u001b[0moutput = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\u001b[90m\u001b[39;49;00m\n",
      "        node       = needle.Tensor([[-1.2294166   0.68918663 -3.6539469 ]\n",
      " [ 0.98761314 -0.05946481 -0.21467207]\n",
      " [ 1.6573914   0.5526084  ...\n",
      " [ 3.1226218   1.3280458  -0.49049693]\n",
      " [ 0.6714237   1.1756382   1.2150986 ]\n",
      " [ 0.9603348  -1.6555276   0.5345011 ]])\n",
      "        out_grad   = needle.Tensor([[ 1.2728198   1.1000712  -0.74093074]\n",
      " [-0.50888014  0.408108    0.53634965]\n",
      " [-2.1118162  -0.27859452 ...\n",
      " [-0.02697281  0.05634464 -0.22966217]\n",
      " [-1.2457664   0.1854648  -0.2667139 ]\n",
      " [-0.31800157 -1.2823775   1.4503455 ]])\n",
      "        self       = <needle.ops.ops_mathematic.Summation object at 0x7f457128bbe0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Summation object at 0x7f457128bbe0>\n",
      "out_grad = needle.Tensor([[ 1.2728198   1.1000712  -0.74093074]\n",
      " [-0.50888014  0.408108    0.53634965]\n",
      " [-2.1118162  -0.27859452 ...\n",
      " [-0.02697281  0.05634464 -0.22966217]\n",
      " [-1.2457664   0.1854648  -0.2667139 ]\n",
      " [-0.31800157 -1.2823775   1.4503455 ]])\n",
      "node = needle.Tensor([[-1.2294166   0.68918663 -3.6539469 ]\n",
      " [ 0.98761314 -0.05946481 -0.21467207]\n",
      " [ 1.6573914   0.5526084  ...\n",
      " [ 3.1226218   1.3280458  -0.49049693]\n",
      " [ 0.6714237   1.1756382   1.2150986 ]\n",
      " [ 0.9603348  -1.6555276   0.5345011 ]])\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        new_shape = \u001b[96mlist\u001b[39;49;00m(node.inputs[\u001b[94m0\u001b[39;49;00m].shape)\u001b[90m\u001b[39;49;00m\n",
      "        axes = \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(new_shape)) \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# add ones to reduced idx for broadcast\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mfor\u001b[39;49;00m axis \u001b[95min\u001b[39;49;00m axes:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "axes       = 2\n",
      "new_shape  = [8, 3, 2]\n",
      "node       = needle.Tensor([[-1.2294166   0.68918663 -3.6539469 ]\n",
      " [ 0.98761314 -0.05946481 -0.21467207]\n",
      " [ 1.6573914   0.5526084  ...\n",
      " [ 3.1226218   1.3280458  -0.49049693]\n",
      " [ 0.6714237   1.1756382   1.2150986 ]\n",
      " [ 0.9603348  -1.6555276   0.5345011 ]])\n",
      "out_grad   = needle.Tensor([[ 1.2728198   1.1000712  -0.74093074]\n",
      " [-0.50888014  0.408108    0.53634965]\n",
      " [-2.1118162  -0.27859452 ...\n",
      " [-0.02697281  0.05634464 -0.22966217]\n",
      " [-1.2457664   0.1854648  -0.2667139 ]\n",
      " [-0.31800157 -1.2823775   1.4503455 ]])\n",
      "self       = <needle.ops.ops_mathematic.Summation object at 0x7f457128bbe0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:241: TypeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes0-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.7175262]]])\n",
      "_A         = array([[[0.7175262]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.7175262]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.7175262]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f4571246a00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.7175262]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f4571246a00>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f4571246580>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f4571246580>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f4571246a00>\n",
      "a = NDArray([[[0.7175262]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.7175262]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f4571246a00>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:162: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes0-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 1), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046...1.4355507   0.1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]])\n",
      "_A         = array([[[-1.1380908 ,  0.45943186,  0.8649256 , -0.5685719 ,\n",
      "         -0.13818243,  0.02258228],\n",
      "        [ 0.7159469 ,...29 ],\n",
      "        [-0.20496817, -0.13278429, -0.44021094,  0.8286478 ,\n",
      "          0.04491751,  1.9707588 ]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046...1.4355507   0.1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.428004...4355507   0.1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457122e820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.428004...4355507   0.1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f457122e820>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457122ee50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457122ee50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f457122e820>\n",
      "a = NDArray([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046   0.0...1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.1380908   0.45943186  0.8649256  -0.5685719  -0.13818243\n",
      "    0.02258228]\n",
      "  [ 0.7159469   1.4280046   0.0...1892495\n",
      "   -1.1416229 ]\n",
      "  [-0.20496817 -0.13278429 -0.44021094  0.8286478   0.04491751\n",
      "    1.9707588 ]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457122e820>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:162: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes1-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 2), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.63587433]]])\n",
      "_A         = array([[[-0.63587433]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.63587433]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.63587433]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f45719ae1c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.63587433]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f45719ae1c0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f45719ae550>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f45719ae550>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f45719ae1c0>\n",
      "a = NDArray([[[-0.63587433]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.63587433]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f45719ae1c0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:162: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-axes1-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 2), device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 ...0.42731738 -0.6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]])\n",
      "_A         = array([[[-1.3012853 , -0.74705434,  1.2911866 ,  0.6756286 ,\n",
      "         -0.724772  ,  1.0151856 ],\n",
      "        [ 0.65764105,...04 ],\n",
      "        [-1.3640325 , -0.79039156,  0.4250712 ,  1.8339677 ,\n",
      "         -0.00649781,  0.58754677]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cpu()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 ...0.42731738 -0.6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536...42731738 -0.6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457128f9a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536...42731738 -0.6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f457128f9a0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457128fdf0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457128fdf0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f457128f9a0>\n",
      "a = NDArray([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 -0.294...6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-1.3012853  -0.74705434  1.2911866   0.6756286  -0.724772\n",
      "    1.0151856 ]\n",
      "  [ 0.65764105 -0.20629536 -0.294...6703581\n",
      "    1.1364204 ]\n",
      "  [-1.3640325  -0.79039156  0.4250712   1.8339677  -0.00649781\n",
      "    0.58754677]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457128f9a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:162: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.8915784]]])\n",
      "_A         = array([[[-0.8915784]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.8915784]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.8915784]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457131c460>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.8915784]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f457131c460>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457131cd60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457131cd60>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f457131c460>\n",
      "a = NDArray([[[-0.8915784]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, a.ndim - \u001b[94m2\u001b[39;49;00m, a.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.8915784]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457131c460>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  ....5810264   0.57262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]])\n",
      "_A         = array([[[-0.23002987,  0.42615405,  0.42854363,  0.02079353,\n",
      "         -0.598037  , -0.30859315],\n",
      "        [ 1.2798208 ,...38 ],\n",
      "        [ 0.8136079 ,  1.3184868 ,  0.17044292,  0.13645671,\n",
      "         -0.42629206, -0.19551516]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  ....5810264   0.57262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768 ...810264   0.57262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f45712c6a30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768 ...810264   0.57262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f45712c6a30>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f45712c6df0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f45712c6df0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f45712c6a30>\n",
      "a = NDArray([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  -1.343...7262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]], device=cpu())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, a.ndim - \u001b[94m2\u001b[39;49;00m, a.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.23002987  0.42615405  0.42854363  0.02079353 -0.598037\n",
      "   -0.30859315]\n",
      "  [ 1.2798208   1.3020768  -1.343...7262874\n",
      "   -0.2943638 ]\n",
      "  [ 0.8136079   1.3184868   0.17044292  0.13645671 -0.42629206\n",
      "   -0.19551516]]], device=cpu())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f45712c6a30>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.06534544]]])\n",
      "_A         = array([[[0.06534544]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.06534544]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.06534544]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f4571263b80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.06534544]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f4571263b80>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f4571263f40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f4571263f40>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f4571263b80>\n",
      "a = NDArray([[[0.06534544]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.06534544]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f4571263b80>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:162: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.9888552...1.3010299  -0.60691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]])\n",
      "_A         = array([[[-0.91022843,  0.64738804, -1.7764558 ,  1.5134803 ,\n",
      "          0.69402885,  0.09445285],\n",
      "        [ 1.1275321 ,...86 ],\n",
      "        [ 0.8013789 ,  0.98249215,  0.06927756,  2.1394844 ,\n",
      "          1.3729885 , -1.2533095 ]]], dtype=float32)\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 1)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.9888552...1.3010299  -0.60691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.988855...3010299  -0.60691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f4571323f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.988855...3010299  -0.60691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f4571323f10>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f4571323370>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f4571323370>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f4571323f10>\n",
      "a = NDArray([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.98885524  0.1...0691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.91022843  0.64738804 -1.7764558   1.5134803   0.69402885\n",
      "    0.09445285]\n",
      "  [ 1.1275321  -0.98885524  0.1...0691965\n",
      "   -1.2607186 ]\n",
      "  [ 0.8013789   0.98249215  0.06927756  2.1394844   1.3729885\n",
      "   -1.2533095 ]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f4571323f10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:162: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = (0, 2), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.69669217]]])\n",
      "_A         = array([[[-0.69669217]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.69669217]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.69669217]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457129eb20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.69669217]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f457129eb20>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457129e700>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457129e700>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f457129eb20>\n",
      "a = NDArray([[[-0.69669217]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.69669217]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457129eb20>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:162: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = (0, 2), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311....9787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]])\n",
      "_A         = array([[[-0.93575615, -0.61421335, -0.45289811,  0.96114016,\n",
      "          0.23405291, -0.8781786 ],\n",
      "        [-0.40377602,...342],\n",
      "        [ 1.1850599 , -0.7730635 ,  1.5469892 , -0.71304744,\n",
      "          0.92298496, -0.52088904]]], dtype=float32)\n",
      "axes       = (0, 2)\n",
      "device     = cuda()\n",
      "np_axes    = (0, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311....9787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]])\n",
      "        axes       = (0, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.361931...787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f45713e5e20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.361931...787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f45713e5e20>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f45713e5610>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f45713e5610>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f45713e5e20>\n",
      "a = NDArray([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311  -0.0...266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311  -0.0...266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f45713e5e20>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:162: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.03809374]]])\n",
      "_A         = array([[[0.03809374]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.03809374]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.03809374]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457130b970>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.03809374]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f457130b970>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457130b850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457130b850>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f457130b970>\n",
      "a = NDArray([[[0.03809374]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, a.ndim - \u001b[94m2\u001b[39;49;00m, a.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[0.03809374]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457130b970>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            np_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.0556547...1.862709   -0.3764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]])\n",
      "_A         = array([[[ 0.9778864 ,  0.07981496, -1.6533154 , -0.28263307,\n",
      "         -0.51364565,  0.08389966],\n",
      "        [-0.5758165 ,...7  ],\n",
      "        [ 0.11819558,  0.84113216, -0.40530366, -0.4401012 ,\n",
      "         -0.16526793, -0.31185058]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:177: in transpose\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.0556547...1.862709   -0.3764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.055654...862709   -0.3764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]]),)\n",
      "        self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457124d4c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.055654...862709   -0.3764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]]),)\n",
      "        op         = <needle.ops.ops_mathematic.Transpose object at 0x7f457124d4c0>\n",
      "        tensor     = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457124d730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[AttributeError(\"module 'needle.backend_ndarray' has no attribute 'swapaxes'\") raised in repr()] Tensor object at 0x7f457124d730>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.ops_mathematic.Transpose object at 0x7f457124d4c0>\n",
      "a = NDArray([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.05565479 -0.7...764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]], device=cuda())\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m], \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mreturn\u001b[39;49;00m array_api.swapaxes(a, a.ndim - \u001b[94m2\u001b[39;49;00m, a.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\u001b[0m\n",
      "\n",
      "a          = NDArray([[[ 0.9778864   0.07981496 -1.6533154  -0.28263307 -0.51364565\n",
      "    0.08389966]\n",
      "  [-0.5758165  -0.05565479 -0.7...764521\n",
      "    2.638757  ]\n",
      "  [ 0.11819558  0.84113216 -0.40530366 -0.4401012  -0.16526793\n",
      "   -0.31185058]]], device=cuda())\n",
      "self       = <needle.ops.ops_mathematic.Transpose object at 0x7f457124d4c0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:164: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_logsumexp[cpu-shape0-None] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[0.26994687]]])\n",
      "A_t        = tensor([[[0.2699]]])\n",
      "_A         = array([[[0.26994687]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "t_axes     = (0, 1, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:55: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[0.26994687]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[0.26994687]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45712c6df0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[0.26994687]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45712c6df0>\n",
      "        tensor     = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45712c64f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45712c64f0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: in compute\n",
      "    \u001b[0mmax_Z_original = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[0.26994687]]], device=cpu())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45712c6df0>\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:200: in amax\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        args       = (NDArray([[[0.26994687]]], device=cpu()),)\n",
      "        dispatcher = <function _amax_dispatcher at 0x7f461081c790>\n",
      "        implementation = <function amax at 0x7f461081c9d0>\n",
      "        kwargs     = {'axis': None, 'keepdims': True}\n",
      "        public_api = <function amax at 0x7f461081ca60>\n",
      "        relevant_args = (NDArray([[[0.26994687]]], device=cpu()), None)\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:2820: in amax\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[0.26994687]]], device=cpu())\n",
      "        axis       = None\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[0.26994687]]], device=cpu()), ufunc = <ufunc 'maximum'>\n",
      "method = 'max', axis = None, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[0.26994687]]], device=cpu())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = None\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[0.26994687]]], device=cpu())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[0.26994687]]], device=cpu())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:84: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]])\n",
      "A_t        = tensor([[ 2.0157,  0.2940,  1.3641],\n",
      "        [ 1.5222,  0.7832, -0.1521],\n",
      "        [ 1.6932,  1.2563,  0.1421],\n",
      "        [ 1.7393,  0.4600, -1.6940],\n",
      "        [-1.1900, -1.3298,  1.6369]])\n",
      "_A         = array([[ 2.0156775 ,  0.29399768,  1.3641229 ],\n",
      "       [ 1.522246  ,  0.7832116 , -0.15208119],\n",
      "       [ 1.6932101 ,  ...1299 ],\n",
      "       [ 1.7393255 ,  0.46002465, -1.6939654 ],\n",
      "       [-1.1899973 , -1.3297868 ,  1.6368964 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:55: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f457038d3a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f457038d3a0>\n",
      "        tensor     = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f457038d310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f457038d310>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: in compute\n",
      "    \u001b[0mmax_Z_original = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f457038d3a0>\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:200: in amax\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        args       = (NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu()),)\n",
      "        dispatcher = <function _amax_dispatcher at 0x7f461081c790>\n",
      "        implementation = <function amax at 0x7f461081c9d0>\n",
      "        kwargs     = {'axis': 0, 'keepdims': True}\n",
      "        public_api = <function amax at 0x7f461081ca60>\n",
      "        relevant_args = (NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu()), None)\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:2820: in amax\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())\n",
      "        axis       = 0\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = 0, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ ... 1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = 0\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ 1.6932101   1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[ 2.0156775   0.29399768  1.3641229 ]\n",
      " [ 1.522246    0.7832116  -0.15208119]\n",
      " [ ... 1.2563031   0.1421299 ]\n",
      " [ 1.7393255   0.46002465 -1.6939654 ]\n",
      " [-1.1899973  -1.3297868   1.6368964 ]], device=cpu())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:84: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1....8]\n",
      "  [ 0.66724986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]])\n",
      "A_t        = tensor([[[-1.4058, -1.3156],\n",
      "         [-0.8430, -0.4802],\n",
      "         [-0.4784,  0.9575]],\n",
      "\n",
      "        [[-1.0233, -1.1449],\n",
      "...         [ 0.6672, -0.0410]],\n",
      "\n",
      "        [[-1.6006, -0.4157],\n",
      "         [ 1.3384,  0.9067],\n",
      "         [ 0.1184, -0.3027]]])\n",
      "_A         = array([[[-1.4058412 , -1.3155643 ],\n",
      "        [-0.8429701 , -0.48019046],\n",
      "        [-0.47839305,  0.95748687]],\n",
      "\n",
      "       [...  [[-1.6006118 , -0.41570628],\n",
      "        [ 1.3384075 ,  0.9066571 ],\n",
      "        [ 0.11837886, -0.30268484]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:55: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1....8]\n",
      "  [ 0.66724986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1...\n",
      "  [ 0.66724986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f4571a1ce80>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1...\n",
      "  [ 0.66724986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f4571a1ce80>\n",
      "        tensor     = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f4571a1cd30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f4571a1cd30>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: in compute\n",
      "    \u001b[0mmax_Z_original = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.144900...986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f4571a1ce80>\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:200: in amax\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        args       = (NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.14490...6 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu()),)\n",
      "        dispatcher = <function _amax_dispatcher at 0x7f461081c790>\n",
      "        implementation = <function amax at 0x7f461081c9d0>\n",
      "        kwargs     = {'axis': 1, 'keepdims': True}\n",
      "        public_api = <function amax at 0x7f461081ca60>\n",
      "        relevant_args = (NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.14490...04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu()), None)\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:2820: in amax\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.144900...986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())\n",
      "        axis       = 1\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.144900...986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = 1, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.9574868...86 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = 1\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.95748687]]\n",
      "\n",
      " [[-1.0232737  -1.144900...986 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[-1.4058412  -1.3155643 ]\n",
      "  [-0.8429701  -0.48019046]\n",
      "  [-0.47839305  0.9574868...86 -0.04097541]]\n",
      "\n",
      " [[-1.6006118  -0.41570628]\n",
      "  [ 1.3384075   0.9066571 ]\n",
      "  [ 0.11837886 -0.30268484]]], device=cpu())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:84: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.... ]\n",
      "  [-0.8324522  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]])\n",
      "A_t        = tensor([[[ 0.6371, -0.1442],\n",
      "         [-1.1688, -0.5101],\n",
      "         [-2.6889, -0.6259]],\n",
      "\n",
      "        [[-1.3568,  1.1981],\n",
      "...         [-0.8325, -0.3871]],\n",
      "\n",
      "        [[-0.4447, -0.2915],\n",
      "         [ 2.9369,  0.4590],\n",
      "         [ 0.8437, -0.1112]]])\n",
      "_A         = array([[[ 0.63712746, -0.14424577],\n",
      "        [-1.1687807 , -0.5100969 ],\n",
      "        [-2.688898  , -0.62585706]],\n",
      "\n",
      "       [...  [[-0.4446728 , -0.29152763],\n",
      "        [ 2.936914  ,  0.4590318 ],\n",
      "        [ 0.8436789 , -0.1111581 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:55: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.... ]\n",
      "  [-0.8324522  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1...\n",
      "  [-0.8324522  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45713b5be0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1...\n",
      "  [-0.8324522  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45713b5be0>\n",
      "        tensor     = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45713b5ac0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45713b5ac0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: in compute\n",
      "    \u001b[0mmax_Z_original = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.198088...22  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45713b5be0>\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:200: in amax\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        args       = (NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.19808...  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu()),)\n",
      "        dispatcher = <function _amax_dispatcher at 0x7f461081c790>\n",
      "        implementation = <function amax at 0x7f461081c9d0>\n",
      "        kwargs     = {'axis': 2, 'keepdims': True}\n",
      "        public_api = <function amax at 0x7f461081ca60>\n",
      "        relevant_args = (NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.19808...38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu()), None)\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:2820: in amax\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.198088...22  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())\n",
      "        axis       = 2\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.198088...22  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = 2, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.6258570...2  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = 2\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.62585706]]\n",
      "\n",
      " [[-1.3568047   1.198088...22  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[ 0.63712746 -0.14424577]\n",
      "  [-1.1687807  -0.5100969 ]\n",
      "  [-2.688898   -0.6258570...2  -0.38714957]]\n",
      "\n",
      " [[-0.4446728  -0.29152763]\n",
      "  [ 2.936914    0.4590318 ]\n",
      "  [ 0.8436789  -0.1111581 ]]], device=cpu())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:84: TypeError\n",
      "\u001b[31m\u001b[1m_______________________ test_logsumexp[cuda-shape0-None] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[1.3820337]]])\n",
      "A_t        = tensor([[[1.3820]]])\n",
      "_A         = array([[[1.3820337]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "t_axes     = (0, 1, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:55: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[1.3820337]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[1.3820337]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45703edf10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[1.3820337]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45703edf10>\n",
      "        tensor     = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45703ed790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45703ed790>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: in compute\n",
      "    \u001b[0mmax_Z_original = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[1.3820337]]], device=cuda())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45703edf10>\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:200: in amax\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        args       = (NDArray([[[1.3820337]]], device=cuda()),)\n",
      "        dispatcher = <function _amax_dispatcher at 0x7f461081c790>\n",
      "        implementation = <function amax at 0x7f461081c9d0>\n",
      "        kwargs     = {'axis': None, 'keepdims': True}\n",
      "        public_api = <function amax at 0x7f461081ca60>\n",
      "        relevant_args = (NDArray([[[1.3820337]]], device=cuda()), None)\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:2820: in amax\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[1.3820337]]], device=cuda())\n",
      "        axis       = None\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[1.3820337]]], device=cuda()), ufunc = <ufunc 'maximum'>\n",
      "method = 'max', axis = None, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[1.3820337]]], device=cuda())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = None\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[1.3820337]]], device=cuda())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[1.3820337]]], device=cuda())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:84: TypeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]])\n",
      "A_t        = tensor([[ 0.5095, -0.1391, -0.4883],\n",
      "        [-1.4052,  0.2294, -0.4997],\n",
      "        [ 0.0422, -1.1866, -0.4498],\n",
      "        [-0.4322,  0.6269, -1.8056],\n",
      "        [ 0.6719, -0.0803,  1.6619]])\n",
      "_A         = array([[ 0.5095096 , -0.13911654, -0.48825276],\n",
      "       [-1.4051776 ,  0.22937068, -0.49968913],\n",
      "       [ 0.042238  , -...7797 ],\n",
      "       [-0.432245  ,  0.6268619 , -1.8056363 ],\n",
      "       [ 0.67194265, -0.08028104,  1.6619049 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:55: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45703682e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45703682e0>\n",
      "        tensor     = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f4570368c40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f4570368c40>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: in compute\n",
      "    \u001b[0mmax_Z_original = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45703682e0>\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:200: in amax\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        args       = (NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda()),)\n",
      "        dispatcher = <function _amax_dispatcher at 0x7f461081c790>\n",
      "        implementation = <function amax at 0x7f461081c9d0>\n",
      "        kwargs     = {'axis': 0, 'keepdims': True}\n",
      "        public_api = <function amax at 0x7f461081ca60>\n",
      "        relevant_args = (NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda()), None)\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:2820: in amax\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())\n",
      "        axis       = 0\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = 0, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ ...1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = 0\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ 0.042238   -1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[ 0.5095096  -0.13911654 -0.48825276]\n",
      " [-1.4051776   0.22937068 -0.49968913]\n",
      " [ ...1.1865976  -0.4497797 ]\n",
      " [-0.432245    0.6268619  -1.8056363 ]\n",
      " [ 0.67194265 -0.08028104  1.6619049 ]], device=cuda())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:84: TypeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-...5e-01]]\n",
      "\n",
      " [[-7.08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]])\n",
      "A_t        = tensor([[[-9.0702e-01, -3.6689e-01],\n",
      "         [ 3.3903e-01, -8.2990e-01],\n",
      "         [ 2.3796e-01,  3.0322e-01]],\n",
      "\n",
      "     ...01]],\n",
      "\n",
      "        [[-7.0857e-01, -7.2344e-01],\n",
      "         [ 1.0095e-01, -4.2828e-01],\n",
      "         [ 1.3839e+00, -5.5759e-01]]])\n",
      "_A         = array([[[-9.07018244e-01, -3.66889060e-01],\n",
      "        [ 3.39028060e-01, -8.29904377e-01],\n",
      "        [ 2.37964749e-01,  3.0...23435521e-01],\n",
      "        [ 1.00946397e-01, -4.28279608e-01],\n",
      "        [ 1.38385820e+00, -5.57593405e-01]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:55: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-...5e-01]]\n",
      "\n",
      " [[-7.08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e...-01]]\n",
      "\n",
      " [[-7.08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45713324f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e...-01]]\n",
      "\n",
      " [[-7.08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45713324f0>\n",
      "        tensor     = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45713321f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45713321f0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: in compute\n",
      "    \u001b[0mmax_Z_original = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "\n",
      "...08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f45713324f0>\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:200: in amax\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        args       = (NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "...571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda()),)\n",
      "        dispatcher = <function _amax_dispatcher at 0x7f461081c790>\n",
      "        implementation = <function amax at 0x7f461081c9d0>\n",
      "        kwargs     = {'axis': 1, 'keepdims': True}\n",
      "        public_api = <function amax at 0x7f461081ca60>\n",
      "        relevant_args = (NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "...3e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda()), None)\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:2820: in amax\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "\n",
      "...08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())\n",
      "        axis       = 1\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "\n",
      "...08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = 1, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.379...8571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = 1\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.37964749e-01  3.03222299e-01]]\n",
      "\n",
      "...08571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[-9.07018244e-01 -3.66889060e-01]\n",
      "  [ 3.39028060e-01 -8.29904377e-01]\n",
      "  [ 2.379...8571613e-01 -7.23435521e-01]\n",
      "  [ 1.00946397e-01 -4.28279608e-01]\n",
      "  [ 1.38385820e+00 -5.57593405e-01]]], device=cuda())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:84: TypeError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",
      "        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            t_axes = axes\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "A          = needle.Tensor([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.... ]\n",
      "  [-2.032398   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]])\n",
      "A_t        = tensor([[[ 1.1967, -1.5918],\n",
      "         [-0.4074,  2.0170],\n",
      "         [ 0.3955, -0.6189]],\n",
      "\n",
      "        [[ 1.0398, -1.0497],\n",
      "...         [-2.0324, -0.6086]],\n",
      "\n",
      "        [[-1.9792,  0.4210],\n",
      "         [-0.6725, -0.5483],\n",
      "         [ 0.2719, -1.1673]]])\n",
      "_A         = array([[[ 1.196672  , -1.5917976 ],\n",
      "        [-0.40744546,  2.01703   ],\n",
      "        [ 0.3955127 , -0.6189291 ]],\n",
      "\n",
      "       [...  [[-1.9791524 ,  0.42104447],\n",
      "        [-0.67249846, -0.5482779 ],\n",
      "        [ 0.2719008 , -1.167323  ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:55: in logsumexp\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\u001b[90m\u001b[39;49;00m\n",
      "        a          = needle.Tensor([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.... ]\n",
      "  [-2.032398   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n",
      "        args       = (needle.Tensor([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1...\n",
      "  [-2.032398   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]]),)\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f4571237fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n",
      "    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n",
      "        inputs     = (needle.Tensor([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1...\n",
      "  [-2.032398   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]]),)\n",
      "        op         = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f4571237fd0>\n",
      "        tensor     = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45712375e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n",
      "        self       = <[TypeError(\"max() got an unexpected keyword argument 'out'\") raised in repr()] Tensor object at 0x7f45712375e0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops/ops_logarithmic.py\u001b[0m:32: in compute\n",
      "    \u001b[0mmax_Z_original = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes, keepdims=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        Z          = NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.049749...   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())\n",
      "        self       = <needle.ops.ops_logarithmic.LogSumExp object at 0x7f4571237fd0>\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:200: in amax\n",
      "    \u001b[0m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        args       = (NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.04974... -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda()),)\n",
      "        dispatcher = <function _amax_dispatcher at 0x7f461081c790>\n",
      "        implementation = <function amax at 0x7f461081c9d0>\n",
      "        kwargs     = {'axis': 2, 'keepdims': True}\n",
      "        public_api = <function amax at 0x7f461081ca60>\n",
      "        relevant_args = (NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.04974...086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda()), None)\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:2820: in amax\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, axis, \u001b[94mNone\u001b[39;49;00m, out,\u001b[90m\u001b[39;49;00m\n",
      "        a          = NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.049749...   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())\n",
      "        axis       = 2\n",
      "        initial    = <no value>\n",
      "        keepdims   = True\n",
      "        out        = None\n",
      "        where      = <no value>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.049749...   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())\n",
      "ufunc = <ufunc 'maximum'>, method = 'max', axis = 2, dtype = None, out = None\n",
      "kwargs = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction = <bound method NDArray.max of NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291...  -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapreduction\u001b[39;49;00m(obj, ufunc, method, axis, dtype, out, **kwargs):\u001b[90m\u001b[39;49;00m\n",
      "        passkwargs = {k: v \u001b[94mfor\u001b[39;49;00m k, v \u001b[95min\u001b[39;49;00m kwargs.items()\u001b[90m\u001b[39;49;00m\n",
      "                      \u001b[94mif\u001b[39;49;00m v \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m np._NoValue}\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mtype\u001b[39;49;00m(obj) \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m mu.ndarray:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                reduction = \u001b[96mgetattr\u001b[39;49;00m(obj, method)\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mexcept\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mpass\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# This branch is needed for reductions like any which don't\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# support a dtype.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mif\u001b[39;49;00m dtype \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">                   \u001b[94mreturn\u001b[39;49;00m reduction(axis=axis, out=out, **passkwargs)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                   TypeError: max() got an unexpected keyword argument 'out'\u001b[0m\n",
      "\n",
      "axis       = 2\n",
      "dtype      = None\n",
      "kwargs     = {'initial': <no value>, 'keepdims': True, 'where': <no value>}\n",
      "method     = 'max'\n",
      "obj        = NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291 ]]\n",
      "\n",
      " [[ 1.0397973  -1.049749...   -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())\n",
      "out        = None\n",
      "passkwargs = {'keepdims': True}\n",
      "reduction  = <bound method NDArray.max of NDArray([[[ 1.196672   -1.5917976 ]\n",
      "  [-0.40744546  2.01703   ]\n",
      "  [ 0.3955127  -0.6189291...  -0.6086498 ]]\n",
      "\n",
      " [[-1.9791524   0.42104447]\n",
      "  [-0.67249846 -0.5482779 ]\n",
      "  [ 0.2719008  -1.167323  ]]], device=cuda())>\n",
      "ufunc      = <ufunc 'maximum'>\n",
      "\n",
      "\u001b[1m\u001b[31m../miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m:84: TypeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cpu-shape0-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cpu-shape1-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cuda-shape0-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cuda-shape1-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cpu-shape0-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cpu-shape0-subtract]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'add'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cpu-shape1-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cpu-shape1-subtract]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'add'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cuda-shape0-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cuda-shape0-subtract]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'add'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cuda-shape1-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'divide'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cuda-shape1-subtract]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'add'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_power[cpu-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_power[cpu-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_power[cuda-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_power[cuda-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'power'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cpu-shape0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cpu-shape1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cuda-shape0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cuda-shape1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cuda-shape0]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cuda-shape1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape0-0-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape1-0-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape2-2-5]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape0-0-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape1-0-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape2-2-5]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape0-0-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape1-0-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape2-2-5]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape0-0-1]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape1-0-2]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape2-2-5]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape1-0]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape2-1]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape3-2]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape1-0]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape2-1]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape3-2]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes0-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes0-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes1-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-axes1-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes0-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes0-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes1-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes1-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'swapaxes'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape0-None]\u001b[0m - TypeError: max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape1-0]\u001b[0m - TypeError: max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape2-1]\u001b[0m - TypeError: max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape3-2]\u001b[0m - TypeError: max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape0-None]\u001b[0m - TypeError: max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape1-0]\u001b[0m - TypeError: max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape2-1]\u001b[0m - TypeError: max() got an unexpected keyword argument 'out'\n",
      "\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape3-2]\u001b[0m - TypeError: max() got an unexpected keyword argument 'out'\n",
      "\u001b[31m================ \u001b[31m\u001b[1m62 failed\u001b[0m, \u001b[32m56 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[31m in 6.03s\u001b[0m\u001b[31m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"nd_backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_nd_backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CIFAR-10 dataset [10 points]\n",
    "\n",
    "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images. \n",
    "\n",
    "Start by implementing the `__init__` function in the `CIFAR10Dataset` class in `python/needle/data/datasets/cifar10_dataset.py`. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
    "\n",
    "Copy `python/needle/data/data_transforms.py` and `python/needle/data/data_basic.py` from previous homeworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Convolutional neural network [40 points]\n",
    "\n",
    "Here's an outline of what you will do in this task.\n",
    "\n",
    "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
    "- `flip`\n",
    "- `pad`\n",
    "\n",
    "In `python/needle/ops_mathematic.py`, implement (forward and backward):\n",
    "- `Flip`\n",
    "- `Dilate`\n",
    "- `UnDilate`\n",
    "- `Conv`\n",
    "\n",
    "In `python/needle/nn/nn_conv.py`, implement:\n",
    "- `Conv`\n",
    "\n",
    "In `apps/models.py`, fill in the `ResNet9` class.  \n",
    "\n",
    "In `apps/simple_ml.py`, fill in:\n",
    "- `epoch_general_cifar10`,\n",
    "- `train_cifar10`\n",
    "- `evaluate_cifar10`\n",
    "\n",
    "We have provided a `BatchNorm2d` implementation in `python/needle/nn/nn_basic.py` for you as a wrapper around your previous `BatchNorm1d` implementation. \n",
    "\n",
    "**Note**: Remember to copy the solution of `nn_basic.py` from previous homework, make sure to not overwrite the `BatchNorm2d` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ndarrays\n",
    "\n",
    "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
    "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
    "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3). \n",
    "\n",
    "Padding is also required for the backward pass of convolution.\n",
    "\n",
    "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
    "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
    "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
    "\n",
    "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"pad_forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping ndarrays & FlipOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
    "# i.e., ignoring strides\n",
    "def raw_data(X):\n",
    "    X = np.array(X) # copy, thus compact X\n",
    "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
    "\n",
    "# Xold and Xnew should reference the same underlying data\n",
    "def offset(Xold, Xnew):\n",
    "    assert Xold.itemsize == Xnew.itemsize\n",
    "    # compare addresses to the beginning of the arrays\n",
    "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
    "\n",
    "def strides(X):\n",
    "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
    "\n",
    "def format_array(X, shape):\n",
    "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i+n] for i in range(0, len(l), n))\n",
    "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
    "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
    "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
    "    return '  '.join(a)\n",
    "\n",
    "def inspect_array(X, *, is_a_copy_of):\n",
    "    # compacts X, then reads it off in order\n",
    "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
    "    # compares address of X to copy_of, thus finding X's offset\n",
    "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
    "    print('Strides: %s' % strides(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
    "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
    "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
    "\n",
    "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
    "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
    "\n",
    "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
    "\n",
    "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
    "\n",
    "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this array as reference for the other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(1, 25).reshape(3, 2, 4)\n",
    "inspect_array(A, is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you flip the array along the last axis below. \n",
    "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
    "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
    "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
    "\n",
    "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
    "to copy this behavior in our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to infer the more general algorithm for computing the offset given the axis to flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens when we flip _all_ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1,2)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flip just axes 1 and 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset is 20. Looking back on our previous offset computations, do you notice something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
    "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops.py`; note that these should be extremely short.\n",
    "\n",
    "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
    "\n",
    "Also, if you want to instead add a `flip` operator on the CPU/CUDA backends, that's also okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"flip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
    "\n",
    "\n",
    "Implement `Dilate` in `ops.py`. This function takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"dilate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit new ops (flip/dilation) to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution forward\n",
    "\n",
    "Implement the forward pass of 2D multi-channel convolution in `ops.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
    "\n",
    "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
    "\n",
    "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2). \n",
    "\n",
    "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
    "\n",
    "We recommend implementing convolution without stride first, ensuring you pass some of the tests below, and then adding in stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
    "\n",
    "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
    "\n",
    "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
    "\n",
    "`X.grad = out_grad @ W.transpose` \\\n",
    "`W.grad = X.transpose @ out_grad`\n",
    "\n",
    "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
    "\n",
    "`X.grad = conv(out_grad, W)` \\\n",
    "`W.grad = conv(X, out_grad)`\n",
    "\n",
    "In which the \"\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
    "\n",
    "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
    "\n",
    "Summarizing some hints for both `X.grad` and `W.grad`:\n",
    "\n",
    "`X.grad`\n",
    "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
    "- `W` should be flipped over both the kernel dimensions\n",
    "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
    "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape \n",
    "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
    "\n",
    "`W.grad`\n",
    "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
    "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
    "    - Consider turning batches into channels via transpose/permute\n",
    "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
    "    - Remember to account for the `padding` argument passed to convolution\n",
    "\n",
    "General tips\n",
    "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
    "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
    "- You can \"permute\" axes with multiple calls to `transpose`\n",
    "\n",
    "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fixing init._calculate_fans for convolution\n",
    "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
    "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
    "\n",
    "**You will need to edit your `kaiming_uniform` in `python/needle/init/init_initializers.py`, etc. init functions to support multidimensional arrays.** In particular, it should support a new `shape` argument which is then passed to, e.g., the underlying `rand` function. Specifically, if the argument `shape` is not None, then ignore `fan_in` and `fan_out` but use the value of `shape` for initializations.\n",
    "\n",
    "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing nn.Conv\n",
    "\n",
    "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
    "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways). \n",
    "\n",
    "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
    "\n",
    "- Ensure nn.Conv works for (N, C, H, W) tensors even though we implemented the conv op for (N, H, W, C) tensors\n",
    "- Initialize the (k, k, i, o) weight tensor using Kaiming uniform initialization with default settings\n",
    "- Initialize the (o,) bias tensor using uniform initialization on the interval $\\pm$`1.0/(in_channels * kernel_size**2)**0.5`\n",
    "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
    "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
    "\n",
    "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit nn.Conv to mugrade [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing \"ResNet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
    "\n",
    "In the figure below, before the linear layer, you should \"flatten\" the tensor. You can use the module `Flatten` in `nn_basic.py`, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
    "\n",
    "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
    "\n",
    "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
    "\n",
    "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a ResNet on CIFAR10: (remember to copy the solutions in `python/needle/optim.py` from previous homeworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"train_cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit ResNet9 to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from models import ResNet9\n",
    "from simple_ml import train_cifar10, evaluate_cifar10\n",
    "\n",
    "device = ndl.cpu()\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "dataloader = ndl.data.DataLoader(\\\n",
    "         dataset=dataset,\n",
    "         batch_size=128,\n",
    "         shuffle=True,)\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
    "      lr=0.001, weight_decay=0.001)\n",
    "evaluate_cifar10(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Recurrent neural network [10 points]\n",
    "\n",
    "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
    "\n",
    "In `python/needle/nn_sequence.py`, implement `RNNCell`.\n",
    "\n",
    "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "In `python/needle/nn_sequence.py`, implement `RNN`.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Long short-term memory network [10 points]\n",
    "In `python/needle/nn/nn_sequence.py`, implement `Sigmoid`.\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$\n",
    "\n",
    "In `python/needle/nn/nn_sequence.py`, implement `LSTMCell`.\n",
    "\n",
    "\\begin{align}\n",
    "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
    "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
    "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
    "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
    "c^\\prime &= f * c + i * g \\\\\n",
    "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively. \n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "Now implement `LSTM` in `python/needle/nn/nn_sequence.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{align}\n",
    "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
    "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
    "c_t &= f * c_{(t-1)} + i * g \\\\\n",
    "h_t &= o * \\text{tanh}(c_t)\n",
    "\\end{align},\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively. \n",
    "\n",
    "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Penn Treebank dataset [10 points]\n",
    "\n",
    "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
    "\n",
    "In `python/needle/data/datasets/ptb_dataset.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
    "\n",
    "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
    "\n",
    "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "\n",
    "```\n",
    " a g m s \n",
    " b h n t \n",
    " c i o u \n",
    " d j p v \n",
    " e k q w \n",
    " f l r x \n",
    "```\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
    "\n",
    "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "```\n",
    " a g m s   b h n t \n",
    " b h n t   c i o u \n",
    "```\n",
    "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"ptb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ptb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training a word-level language model [10 points]\n",
    "\n",
    "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
    "\n",
    "First, in `python/needle/nn/nn_sequence.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
    "\n",
    "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of \n",
    "\n",
    "- An embedding layer (which maps word IDs to embeddings) \n",
    "- A sequence model (either RNN or LSTM)\n",
    "- A linear layer (which outputs probabilities of the next word)\n",
    "\n",
    "In `apps/simple_ml.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"language_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your language model on the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_ml import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
    "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
    "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
    "evaluate_ptb(model, train_data, seq_len=40, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
